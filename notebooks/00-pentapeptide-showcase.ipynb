{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Showcase pentapeptide: a PyEMMA walkthrough\n",
    "In this notebook, we introduce the most basic features of PyEMMA. Overall, the notebook serves as an example workflow for analyzing molecular dynamics trajectories. Here, we keep the details to absolutely minimal and refer to the more specialized notebooks fleshing out each topic covered here in more details, including exercises and some theory.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "Maintainers: [@cwehmeyer](https://github.com/cwehmeyer), [@marscher](https://github.com/marscher), [@thempel](https://github.com/thempel), [@psolsson](https://github.com/psolsson)\n",
    "\n",
    "Remember, to\n",
    "- run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;\n",
    "- find the full documentation at [PyEMMA.org](http://www.pyemma.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import mdshare\n",
    "import pyemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input and featurization  [âžœ ðŸ““](01-data-io-and-featurization.ipynb) \n",
    "### Loading data\n",
    "We start our short walkthrough tutorial by loading a topology file (in this case, a PDB) and the trajectory data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdb = mdshare.fetch('pentapeptide-impl-solv.pdb', working_directory='data')\n",
    "files = mdshare.fetch('pentapeptide-*-500ns-impl-solv.xtc', working_directory='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell has downloaded the data from our servers; in general, only strings of file paths need to be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pdb)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As from the beginning it is unknown which feature describes the system best, we start with a broad systematic analysis. For the sake of simplicity, we are only interested in modeling the backbone kinetics. Thus, we only consider features describing the backbone.\n",
    "\n",
    "In PyEMMA, the `featurizer` is a central object that incorporates the system's topology. Features are easily computed by adding the target feature e.g. with `featurizer.add_backbone_torsions()`. We will load backbone torsion angles, backbone heavy atom distances and backbone heavy atom distances. \n",
    "\n",
    "Please note that the structures have been aligned before. Since in that case, we loose track of the periodic box, we have to switch off the `periodic` flag for the distance and torsion angle computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torsions_feat = pyemma.coordinates.featurizer(pdb)\n",
    "torsions_feat.add_backbone_torsions(cossin=True, periodic=False)\n",
    "torsions_data = pyemma.coordinates.load(files, features=torsions_feat)\n",
    "labels = ['backbone\\ntorsions']\n",
    "\n",
    "positions_feat = pyemma.coordinates.featurizer(pdb)\n",
    "positions_feat.add_selection(positions_feat.select_Backbone())\n",
    "positions_data = pyemma.coordinates.load(files, features=positions_feat)\n",
    "labels += ['backbone atom\\npositions']\n",
    "\n",
    "distances_feat = pyemma.coordinates.featurizer(pdb)\n",
    "distances_feat.add_distances(\n",
    "    distances_feat.pairs(distances_feat.select_Backbone(), excluded_neighbors=2), periodic=False)\n",
    "distances_data = pyemma.coordinates.load(files, features=distances_feat)\n",
    "labels += ['backbone atom\\ndistances']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "We will now rank the three featurizations by means of the VAMP2 score which measures the kinetic variance contained in the features. The minimum value of this score is 1, which corresponds to the invariant measure or equilibrium.\n",
    "\n",
    "As we are comparing featurizations with different dimensionality, we use the dimension parameter to provide an upper bound for the number of dynamic processes included in the scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_cv(data, dim, lag, nrep=10, fval=0.5):\n",
    "    with pyemma.util.contexts.settings(show_progress_bars=False):\n",
    "        nval = int(len(data) * fval)\n",
    "        scores = np.zeros(nrep)\n",
    "        for n in range(nrep):\n",
    "            ival = np.random.choice(len(data), size=nval, replace=False)\n",
    "            vamp = pyemma.coordinates.vamp(\n",
    "                [d for i, d in enumerate(data) if i not in ival], lag=lag, dim=dim)\n",
    "            scores[n] = vamp.score([d for i, d in enumerate(data) if i in ival])\n",
    "    return scores\n",
    "\n",
    "\n",
    "dim = 10\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\n",
    "for ax, lag in zip(axes.flat, [5, 10, 20]):\n",
    "    torsions_scores = score_cv(torsions_data, lag=lag, dim=dim)\n",
    "    scores = [torsions_scores.mean()]\n",
    "    errors = [torsions_scores.std()]\n",
    "    positions_scores = score_cv(positions_data, lag=lag, dim=dim)\n",
    "    scores += [positions_scores.mean()]\n",
    "    errors += [positions_scores.std()]\n",
    "    distances_scores = score_cv(distances_data, lag=lag, dim=dim)\n",
    "    scores += [distances_scores.mean()]\n",
    "    errors += [distances_scores.std()]\n",
    "    ax.bar(labels, scores, yerr=errors, color=['C0', 'C1', 'C2'])\n",
    "    ax.set_title(r'lag time $\\tau$={:.1f}ns'.format(lag * 0.1))\n",
    "    if lag == 5:\n",
    "        # save for later\n",
    "        vamp_bars_plot = dict(\n",
    "            labels=labels, scores=scores, errors=errors, dim=dim, lag=lag)\n",
    "axes[0].set_ylabel('VAMP2 score')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the VAMP-2 score is **not** suitable to select the proper lag time as scores for different lag times are not comparable. Here, we just compare different features at each given lag time separately. We consistently find backbone torsions to have a slight advantage.\n",
    "\n",
    "Thus, we add a more detailed VAMP2 score analysis for backbone torsions by varying the dimension parameter for several lag times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1, 2, 5, 10, 20]\n",
    "dims = [i + 1 for i in range(10)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i, lag in enumerate(lags):\n",
    "    scores_ = np.array([score_cv(torsions_data, dim, lag)\n",
    "                        for dim in dims])\n",
    "    scores = np.mean(scores_, axis=1)\n",
    "    errors = np.std(scores_, axis=1, ddof=1)\n",
    "    color = 'C{}'.format(i)\n",
    "    ax.fill_between(dims, scores - errors, scores + errors, alpha=0.3, facecolor=color)\n",
    "    ax.plot(dims, scores, '--o', color=color, label='lag={:.1f}ns'.format(lag * 0.1))\n",
    "ax.legend()\n",
    "ax.set_xlabel('number of dimensions')\n",
    "ax.set_ylabel('VAMP2 score')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that, for lag times above 0.5 ns, using more dimensions does not increase the score, i.e., the first four dimensions contain all relevant information of the slow dynamics.\n",
    "\n",
    "Based on this result, we try a TICA projection with lag time 0.5 ns (5 steps). Please note that this is a modeler's choice based on the best heuristics that is currently available to our knowledge. It might be necessary to re-adjust the TICA lagtime after the MSM estimation. \n",
    "\n",
    "\n",
    "## Coordinate transform and discretization  [âžœ ðŸ““](02-dimension-reduction-and-discretisation.ipynb) \n",
    "### TICA\n",
    "\n",
    "The goal of the next step is to find a function that maps the usually high-dimensional input space into some lower dimensional space that captures the important dynamics. The recommended way of doing so is time-lagged independent component analysis (TICA). We perform TICA (with kinetic map scaling) using the lag time obtained from the VAMP-2 score. \n",
    "\n",
    "Please note the general `PyEMMA` API that applies to all estimators. By calling the TICA estimator with the data (`tica = pyemma.coordinates.tica(torisions_data)`), the estimation is done and an estimator instance returned (`tica`). It contains all the information about the specific entity and maps the input data by calling `tica.get_output()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica = pyemma.coordinates.tica(torsions_data, lag=5)\n",
    "tica_output = tica.get_output()\n",
    "tica_concatenated = np.concatenate(tica_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the marginal and joint distributions of our TICA components by simple histograming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(tica_concatenated, ax=axes[0], ylog=True)\n",
    "pyemma.plots.plot_density(*tica_concatenated[:, :2].T, ax=axes[1], logscale=True)\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the projection yields defined clusters of high densities, that are most likely to be identified as metastable basins. \n",
    "\n",
    "Letâ€™s have a look how one of the trajectories looks like in the space of the first $4$ TICA components. We can see that the TICA components nicely resolve the slow transitions as discrete jumps. Thus, metastability is well-described in this projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, figsize=(12, 5), sharex=True)\n",
    "x = 0.1 * np.arange(tica_output[0].shape[0])\n",
    "for i, (ax, tic) in enumerate(zip(axes.flat, tica_output[0].T)):\n",
    "    ax.plot(x, tic)\n",
    "    ax.set_ylabel('IC {}'.format(i + 1))\n",
    "axes[-1].set_xlabel('time / ns')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization\n",
    "\n",
    "The TICA coordinates are now clustered into a number of discrete states using the $k$-means algorithm. The $k$-means algorithm requires as input the number of clusters. The trajectories are automatically assigned to the cluster centers by calling `cluster.dtrajs`. \n",
    "\n",
    "It is a priori unclear what the optimal number of cluster centers $k$ is. It largely depends on the distribution of our data and on the number of dimensions we use. In the following, we will estimate unvalidated Markov models using different numbers of cluster centers and use the VAMP-2 score (using cross validation) as a heuristics. This approach requires us to guess the MSM lag time, which we set to the TICA lag time of $5$ steps (or $0.5$ ns). Since the clustering algorithm is stochastic, we conduct multiple rounds of discretization at each number of cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_clustercenters = [5, 10, 30, 75, 200, 450]\n",
    "\n",
    "scores = np.zeros((len(n_clustercenters), 5))\n",
    "for n, k in enumerate(n_clustercenters):\n",
    "    for m in range(5):\n",
    "        _cl = pyemma.coordinates.cluster_kmeans(tica_output, k=k, max_iter=50, stride=50)\n",
    "        _msm = pyemma.msm.estimate_markov_model(_cl.dtrajs, 5)\n",
    "        scores[n, m] = _msm.score_cv(_cl.dtrajs, n=1, score_method='VAMP2', score_k=min(10, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lower, upper = pyemma.util.statistics.confidence_interval(scores.T.tolist(), conf=0.9)\n",
    "ax.fill_between(n_clustercenters, lower, upper, alpha=0.3)\n",
    "ax.plot(n_clustercenters, np.mean(scores, axis=1), '-o')\n",
    "ax.semilogx()\n",
    "ax.set_xlabel('# cluster centers')\n",
    "ax.set_ylabel('VAMP-2 scores')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the VAMP-2 score is already saturated at $75$ states. We will use this number for further analysis.\n",
    "\n",
    "As already stated above, the score has been generated using MSMs that were not validated, meaning that the above plot is really just a heuristics. Besides having an optimal score, we want to obtain a model that describe physically interesting states. Thus, the number of states $k$ is often re-adjusted after model inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster = pyemma.coordinates.cluster_kmeans(\n",
    "    tica_output, k=75, max_iter=50, stride=10, fixed_seed=True)\n",
    "dtrajs_concatenated = np.concatenate(cluster.dtrajs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that discretization is a stochastic procedure and, hence, individual runs might yield different results. In order to have congruent results with the interpretations for each run in this example, we are fixing the random seed (not advised in general). \n",
    "\n",
    "We can check the location of our discrete states by plotting them onto the density of our data in the first two TICA dimensions. The cluster centers are contained in the `cluster` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "pyemma.plots.plot_density(\n",
    "    *tica_concatenated[:, :2].T, ax=ax, cbar=False, alpha=0.3)\n",
    "ax.scatter(*cluster.clustercenters[:, :2].T, s=5, c='C1')\n",
    "ax.set_xlabel('IC 1')\n",
    "ax.set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the states are well distributed in TICA space.\n",
    "\n",
    "## MSM estimation and validation  [âžœ ðŸ““](03-msm-estimation-and-validation.ipynb)\n",
    "### Implied timescales\n",
    "The first validation that is usually done when estimating a Markov model is the estimation of implied timescales (ITS) $t_i$. They are computed from the eigenvalues of the Markov transition matrix $\\lambda_i$ by\n",
    "$$\n",
    "t_i = -\\frac{\\tau}{\\ln|\\lambda_i(\\tau)|}\n",
    "$$\n",
    "\n",
    "with $\\tau$ being the lag time. The ITS $t_i$ describes the decorrelation time of the $i$-th process which is independent of the  hyper-parameter $\\tau$. Thus, we look for ITS convergence and chose $\\tau$ accordingly. Please note that the lagtime also represents the time resolution limit of the estimated Markov model.\n",
    "\n",
    "Now, we calculate the ITS with `pyemma.msm.its()` up to a lagtime of $50$ steps, which is equivalent to $5\\,\\mathrm{ns}$ for this dataset ($\\Delta t=0.1\\,\\mathrm{ns}$). The uncertainty of the implied timescales is quantified based upon Markov models sampled according to a Bayesian scheme. If this this is too time-consuming maximum likelihood MSMs can be used instead, by setting the errors keyword argument to `None`.\n",
    "\n",
    "Please note that instead of a single number `lags=50`, an array can be passed to compute the ITS at defined lagtimes. When we pass an integer ($K$) as this value a set of lag times starting from $\\Delta t$ to $K\\Delta t$ will be generated, using a multiplier of 1.5 between successive lag-times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its = pyemma.msm.its(cluster.dtrajs, lags=50, nits=10, errors='bayes')\n",
    "pyemma.plots.plot_implied_timescales(its, units='ns', dt=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid lines correspond to the ITS of maximum likelihood MSMs. The confidence intervals are depicted by the shaded areas. They contain 95% of the samples generated by the Bayesian MSM; the sample means are given by dashed lines.\n",
    "\n",
    "The implied timescales converge quickly. Above $0.5$ ns, the implied timescales of the slowest processes are constant within error. We thus select a lag time of $5$ steps ($0.5$ ns) to build a Markov model. As a quick check we print the fraction of states and counts that are in the active set. \n",
    "\n",
    "Please note the similarity of the `msm` object to the `tica` object. Both are estimator instances and contain all the relevant information from the estimation and methods for validation and further analysis. \n",
    "\n",
    "In order to keep track of our trajectory time step, a `dt_traj` keyword argument can be passed that contains the trajectory time step unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msm = pyemma.msm.bayesian_markov_model(cluster.dtrajs, lag=5, dt_traj='0.1 ns')\n",
    "print('fraction of states used = {:.2f}'.format(msm.active_state_fraction))\n",
    "print('fraction of counts used = {:.2f}'.format(msm.active_count_fraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapman-Kolmogorov test\n",
    "The model is validated with a Chapman-Kolmogorov test. It compares the right and the left side of the Chapman-Kolmogorov equation\n",
    "\n",
    "$$\n",
    "T(k \\tau) = T^k(\\tau)\n",
    "$$\n",
    "\n",
    "with $T$ being the transition matrix. `PyEMMA` automatically estimates a new MSM transition matrix at lagtime $k \\tau$ and propagates the original transition matrix by the $k$-th power. The highest $k$ can be adjusted using the `mlags` keyword argument of `msm.cktest()`.\n",
    "\n",
    "Since we can only inspect the result for a small number of (macro-) states, we use the implied timescales plot as a heuristics to estimate a number of metastable states to test for. We can resolve $3$ slow processes up to lagtimes of $5$ ns. Since the Chapman-Kolmogorov test involves estimations at higher lagtimes, we will attempt to capture those processes choosing  $4$ metastable states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstates = 4\n",
    "cktest = msm.cktest(nstates)\n",
    "pyemma.plots.plot_cktest(cktest, dt=0.1, units='ns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming $4$ metastable states yields a passing Chapman-Kolmogorov test.\n",
    "\n",
    "## MSM spectral analysis [âžœ ðŸ““](04-msm-analysis.ipynb)\n",
    "\n",
    "From the MSM object `msm`, various properties can be obtained. We start the spectral analysis by examining the implied timescales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def its_separation_err(ts, ts_err):\n",
    "    \"\"\"\n",
    "    Error propagation from ITS standard deviation to timescale separation.\n",
    "    \"\"\"\n",
    "    return ts[:-1] / ts[1:] * np.sqrt(\n",
    "        (ts_err[:-1] / ts[:-1])**2 + (ts_err[1:] / ts[1:])**2)\n",
    "\n",
    "\n",
    "nits = 15\n",
    "\n",
    "timescales_mean = msm.sample_mean('timescales', k=nits)\n",
    "timescales_std = msm.sample_std('timescales', k=nits)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].errorbar(\n",
    "    range(1, nits + 1),\n",
    "    timescales_mean, \n",
    "    yerr=timescales_std, \n",
    "    fmt='.', markersize=10)\n",
    "axes[1].errorbar(\n",
    "    range(1, nits),\n",
    "    timescales_mean[:-1] / timescales_mean[1:], \n",
    "    yerr=its_separation_err(\n",
    "        timescales_mean, \n",
    "        timescales_std), \n",
    "    fmt='.', \n",
    "    markersize=10,\n",
    "    color='C0')\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_xticks(range(1, nits + 1))\n",
    "    ax.grid(True, axis='x', linestyle=':')\n",
    "    \n",
    "axes[0].axhline(msm.lag * 0.1, lw=1.5, color='k')\n",
    "axes[0].axhspan(0, msm.lag * 0.1, alpha=0.3, color='k')\n",
    "axes[0].set_xlabel('implied timescale index')\n",
    "axes[0].set_ylabel('implied timescales / ns')\n",
    "axes[1].set_xticks(range(1, nits))\n",
    "axes[1].set_xticklabels(\n",
    "    [\"{:d}/{:d}\".format(k, k + 1) for k in range(1, nits + 2)],\n",
    "    rotation=45)\n",
    "axes[1].set_xlabel('implied timescale indices')\n",
    "axes[1].set_ylabel('timescale separation')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, `PyEMMA` sorts the implied timescales (and their corresponding eigenfunctions) in descending order. From the time-scale separation we can see that the largest time-scale gap, within the time-resolution of the model, is between the 3rd and 4th process, suggesting that $4$ meta-stable states may be a good choice for coarse-graining. We discuss this further below.\n",
    "\n",
    "We go on by analyzing the stationary distribution and the free energy computed over the first two TICA coordinates. The stationary distribution, $\\pi$, is stored in `msm.pi` or  (as an alias) `msm.stationary_distribution`. We compute the free energy landscape by re-weighting the trajectory frames with stationary probabilities from the MSM (returned by  `msm.trajectory_weights()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "pyemma.plots.plot_contour(\n",
    "    *tica_concatenated[:, :2].T,\n",
    "    msm.pi[dtrajs_concatenated],\n",
    "    ax=axes[0],\n",
    "    mask=True,\n",
    "    cbar_label='stationary distribution')\n",
    "pyemma.plots.plot_free_energy(\n",
    "    *tica_concatenated[:, :2].T,\n",
    "    weights=np.concatenate(msm.trajectory_weights()),\n",
    "    ax=axes[1],\n",
    "    legacy=False)\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('IC 1')\n",
    "axes[0].set_ylabel('IC 2')\n",
    "axes[0].set_title('Stationary distribution', fontweight='bold')\n",
    "axes[1].set_title('Reweighted free energy surface', fontweight='bold')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors corresponding to the slowest processes (largest implied time-scales) contain information about what configurational changes are happening on what time-scales. We analyze the slowest processes by inspecting the value of the first four eigenfunctions projected on two the first TICA coordinates. As the first right eigenvector corresponds to the stationary process (equilibrium) it is constant at $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvec = msm.eigenvectors_right()\n",
    "print('The first eigenvector is one: {} (min={}, max={})'.format(\n",
    "    np.allclose(eigvec[:, 0], 1, atol=1e-15), eigvec[:, 0].min(), eigvec[:, 0].max()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    pyemma.plots.plot_contour(\n",
    "        *tica_concatenated[:, :2].T,\n",
    "        eigvec[dtrajs_concatenated, i + 1],\n",
    "        ax=ax,\n",
    "        cmap='PiYG',\n",
    "        cbar_label='{}. right eigenvector'.format(i + 2),\n",
    "        mask=True)\n",
    "    ax.set_xlabel('IC 1')\n",
    "axes[0].set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvector of the MSM contain information about what conformational changes are happening on a certain timescale, governed by the correponding implied timescale. Specifically, conformational states in areas of configuration space at negative-values for a given eigenvector, exchange with corresponding positive regions for the same eigenvector. The relaxation timescale of this exchange process is exactly the implied timescale. Since the eigenvectors were internally sorted according to their eigenvalue, they correspond to the four slowest processes of the implied timescale plot. We see that indeed, the slowest processes occur inbetween the dense clusters in the TICA projection.\n",
    "\n",
    "## PCCA & TPT [âžœ ðŸ““](05-pcca-tpt.ipynb)\n",
    "### Perron cluster cluster analysis\n",
    "Next, we do a PCCA++ coarse graining into a user defined number of macrostates. As already discussed, $4$ is a good choice for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msm.pcca(nstates);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCCA++ algorithm computes the so called memberships, i.e. the probability of each microstate to belong to a given macrostate. In other words, PCCA++ does a fuzzy assignment of the microstates to macrostates which is encoded in the memberships. We can visualize the $4$ membership distributions over the first $2$ TICA dimensions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3), sharex=True, sharey=True)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    pyemma.plots.plot_contour(\n",
    "        *tica_concatenated[:, :2].T,\n",
    "        msm.metastable_distributions[i][dtrajs_concatenated],\n",
    "        ax=ax,\n",
    "        cmap='afmhot_r', \n",
    "        mask=True,\n",
    "        cbar_label='metastable distribution {}'.format(i))\n",
    "    ax.set_xlabel('IC 1')\n",
    "axes[0].set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the membership probabilities roughly match to the basins of the free energy landscape presented above.\n",
    "\n",
    "In some cases, it might be useful to convert these distributions into crisp assignments. This can be computed by taking the argmax of each microstate's memberships to the macrostates. They are contained in `msm.metastable_assignments`. Let's see what this looks like in the first two TICA projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metastable_traj = msm.metastable_assignments[dtrajs_concatenated]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "pyemma.plots.plot_state_map(\n",
    "    *tica_concatenated[:, :2].T, metastable_traj, ax=ax)\n",
    "ax.set_xlabel('IC 1')\n",
    "ax.set_ylabel('IC 2')\n",
    "ax.set_xlim(-2, 8)\n",
    "ax.set_ylim(-2, 8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect from the membership probabilities, PCCA++ has nicely separated our state space in the first two TICA components. Another means of finding metastable states are hidden Markov models (HMMs), they are introduced [here ðŸ““](06-hidden-markov-state-models.ipynb).\n",
    "\n",
    "At this point, we usually want to investigate what molecular structures the identified metastable structures correspond to. We generate a number of representative sample structures for each macrostate and store them into a trajectory file for visual inspection. The following cell writes trajectory files to hard disc. They can be loaded and analyzed with external software packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcca_samples = msm.sample_by_distributions(msm.metastable_distributions, 10)\n",
    "torsions_source = pyemma.coordinates.source(files, features=torsions_feat)\n",
    "pyemma.coordinates.save_trajs(\n",
    "    torsions_source,\n",
    "    pcca_samples,\n",
    "    outfiles=['./data/pcca{}_10samples.pdb'.format(n)\n",
    "              for n in range(msm.n_metastable)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can visualize the structures in this notebook with NGLView. For this, we need to provide a custom function that defines the representations of our molecule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metastable(samples, cmap, selection='not element H'):\n",
    "    \"\"\" visualize metastable states\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples: list of mdtraj.Trajectory objects\n",
    "        each element contains all samples for one metastable state.\n",
    "    cmap: matplotlib.colors.ListedColormap\n",
    "        color map used to visualize metastable states before.\n",
    "    selection: str\n",
    "        which part of the molecule to selection for visualization. For details have a look here:\n",
    "        http://mdtraj.org/latest/examples/atom-selection.html#Atom-Selection-Language\n",
    "    \"\"\"\n",
    "    import nglview\n",
    "    from matplotlib.colors import to_hex\n",
    "\n",
    "    widget = nglview.NGLWidget()\n",
    "    widget.clear_representations()\n",
    "    ref = samples[0]\n",
    "    for i, s in enumerate(samples):\n",
    "        s = s.superpose(ref, atom_indices=s.top.select('resid 2 3 and mass > 2'))\n",
    "        s = s.atom_slice(s.top.select(selection))\n",
    "        comp = widget.add_trajectory(s)\n",
    "        comp.add_licorice()\n",
    "\n",
    "    # this has to be done in a separate loop for whatever reason...\n",
    "    x = np.linspace(0, 1, num=len(samples))\n",
    "    for i, x_ in enumerate(x):\n",
    "        c = to_hex(cmap(x_))\n",
    "        widget.update_licorice(color=c, component=i, repr_index=i)\n",
    "        widget.remove_cartoon(component=i)\n",
    "    return widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to saving the trajectories to disc, we now create a list with `mdtraj.Trajectory` objects that contain samples of our metastable structures. They are visualized with NGLView as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_samples = [pyemma.coordinates.save_traj(files, idist, outfile=None, top=pdb)\n",
    "              for idist in msm.sample_by_distributions(msm.metastable_distributions, 50)]\n",
    "\n",
    "cmap = mpl.cm.get_cmap('viridis', nstates)\n",
    "visualize_metastable(my_samples, cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coarse-grained representation of the dynamics is more directly amenable to human interpretation. Nevertheless, as for the conventional MSM, we can still compute several interesting properties. We start with the stationary distribution which encodes the free energy of the states. This can be achieved by summing all the  contributions to a coarse-grained state $\\mathcal{S}_i$:\n",
    "$$\n",
    "G_i = - k_b T \\ln(\\sum_{j\\in \\mathcal{S}_i} \\pi_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('state\\tÏ€\\t\\tG/kT')\n",
    "for i, s in enumerate(msm.metastable_sets):\n",
    "    p = msm.pi[s].sum()\n",
    "    print('{}\\t{:f}\\t{:f}'.format(i, p, -np.log(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing PCCA metastable states, we can also extract mean first passage times (MFPTs) and transition rates between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "mfpt = np.zeros((nstates, nstates))\n",
    "mfpt_err = np.zeros((nstates, nstates))\n",
    "for i, j in product(range(nstates), repeat=2):\n",
    "    mfpt[i, j] = msm.mfpt(\n",
    "        msm.metastable_sets[i],\n",
    "        msm.metastable_sets[j])\n",
    "    mfpt_err[i, j] = msm.sample_std(\n",
    "        'mfpt',\n",
    "        msm.metastable_sets[i],\n",
    "        msm.metastable_sets[j])\n",
    "\n",
    "from pandas import DataFrame\n",
    "print('MFPT (ns):')\n",
    "DataFrame(np.array(['{:.1f} Â± {:05.1f}'.format(a, b)\n",
    "                    for a, b in zip(mfpt.flat, mfpt_err.flat)]).reshape(nstates, nstates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it will become clear in further sections, metastable state $0$ could be distinguished from the other states by experiment. We can extract the MFPT into (out of) this state from (into) any other state as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = msm.metastable_sets[0]\n",
    "B = np.concatenate(msm.metastable_sets[1:])\n",
    "print('MFPT 0 -> other: ({:6.1f} Â± {:5.1f}) ns'.format(msm.mfpt(A, B), msm.sample_std('mfpt', A, B)))\n",
    "print('MFPT other -> 0: ({:.1f} Â± {:5.1f}) ns'.format(msm.mfpt(B, A), msm.sample_std('mfpt', B, A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MFPT from state $0$ to any other state is very short compared to the other direction, i.e. this state has a comparably short lifetime. \n",
    "\n",
    "### Transition path theory\n",
    "Further, the flux between metastable states can be computed and coarse grained as follows. We chose metastable states $4$ and $3$ as an example between which the flux is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, final = 1, 2\n",
    "A = msm.metastable_sets[start]\n",
    "B = msm.metastable_sets[final]\n",
    "flux = pyemma.msm.tpt(msm, A, B)\n",
    "\n",
    "cg, cgflux = flux.coarse_grain(msm.metastable_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The committor as projected in the first two TICA dimensions can be displayed with a filled contour plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "pyemma.plots.plot_contour(\n",
    "    *tica_concatenated[:, :2].T,\n",
    "    flux.committor[dtrajs_concatenated],\n",
    "    cmap='brg',\n",
    "    ax=ax,\n",
    "    mask=True,\n",
    "    cbar_label=r'committor {} $\\to$ {}'.format(start, final))\n",
    "ax.set_xlim(-2, 8)\n",
    "ax.set_ylim(-2, 8)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the committor is constant within the metastable sets defined above. Transition regions can be identified by committor values $\\tilde{} 0.5$.\n",
    "## Computing experimental observables [âžœ ðŸ““](07-expectations-and-observables.ipynb)\n",
    "\n",
    "Having thoroughly constructed, validated and analysed our MSM above, we may want to take the next step and compare our model to experimental data. `PyEMMA` enables computation of stationary as well as dynamic experimental observables, below we give give some examples of this. We will make use of some external library functionality provided by `MDTraj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdtraj import shrake_rupley, compute_rg\n",
    "\n",
    "#We compute a maximum likelihood MSM for comparison\n",
    "mlmsm = pyemma.msm.estimate_markov_model(cluster.dtrajs, lag=5, dt_traj='0.1 ns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way `PyEMMA` computes all experimental observables relies on us having a value of the observable for each of the Markov states in our MSM. Computing these quantities depends on the experiment in question and may require specific domain knowledege, or involve time-consuming computations. Here we will pre-compute these experimental observables in the following cells. \n",
    "First, we sample 20 representative configurations from each Markov state. Note that 20 representative configurations is not a universal number: a specific observable of interest may require substantially more representative configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pyemma.coordinates.source(files, top=pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_samples = [smpl for smpl in msm.sample_by_state(20)]\n",
    "_samples = [pyemma.coordinates.save_traj(reader, smpl, outfile=None, top=pdb)\n",
    "            for smpl in markov_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we back-compute two experimental observable of each of the sampled configurations using `MDTraj`, and average each of these observables over the Markov states. Each of these averages corresponds to our Markov state proxy of the macrosopic experimental observable we want to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute surface accessible surface area for all samples\n",
    "markov_sasa_all = [shrake_rupley(sample, mode='residue')\n",
    "                   for sample in _samples]\n",
    "# Compute radius of gyration for all samples\n",
    "markov_rg_all = [compute_rg(sample) for sample in _samples]\n",
    "\n",
    "# Average over Markov states for both observables.\n",
    "markov_average_trp_sasa = np.array(markov_sasa_all).mean(axis=1)[:, 0]\n",
    "markov_average_rg = np.array(markov_rg_all).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radius of gyration\n",
    "The radius of gyration $r^2_g(x)$ is a measure of the over-all dimensions of a particles in configuration, $x$. It is a quantity often extracted extracted from light-scattering experiments. In the context of proteins and nucleic acids these experiments often happen on bulk samples and the observables is therefore stationary and averaged by the Boltzmann distribution:\n",
    "$$ \n",
    " R_{g, \\mathrm{obs}}^2 = \\mathcal{Z}^{-1}\\int_{\\Omega} \\mathrm{d}x\\, r_g(x) \\exp{(-\\frac{E(x)}{kT})}.\n",
    "$$\n",
    "This value is also called the _expectation value of $r^2_g(x)$ with respect to the Boltzmann distribution_. Since we have access to the stationary distribution from the Markov model we can approximate this continuous integral by the sum:\n",
    "\n",
    "$$\n",
    " R_{g, \\mathrm{obs}}^2 \\approx \\sum_i \\pi_i \\mathbf{r^2_g}_i \n",
    "$$\n",
    "with\n",
    "$$ \\mathbf{r^2_g}_i  = \\frac{1}{\\pi_i\\mathcal{Z}} \\int_{x\\in S_i} \\mathrm{d}x\\, r^2_g(x)\\exp(-\\beta E(x)). $$\n",
    "The last expression constitutes the average of the experimental observable ($r^2_g(x)$) confined to each of our Markov states individually. \n",
    "\n",
    "Recall, that we pre-computed the vector $\\mathbf{r^2_g}=\\{\\mathbf{r^2_g}_i\\}$ as `markov_average_rg`, above. Using this vector and our estimated Markov model we can compute the expectation value using the `expectation` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average radius of gyration of penta-peptide is'\n",
    "      ' {:.3f} nm'.format(msm.expectation(markov_average_rg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a Bayesian MSM estimated we can also compute the uncertainty in our prediction of the observable as standard deviations or confidence intervals. To achieve this we use the methods `sample_std` and `sample_conf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The standard deviation of our prediction of the average radius of gyration'\n",
    "      ' of penta-peptide is {:.9f} nm'.format(\n",
    "          msm.sample_std('expectation', markov_average_rg)))\n",
    "print('The {:d}% CI of our prediction of the average radius of gyration of'\n",
    "      ' penta-peptide have the bounds ({:.5f}, {:.5f})'.format(\n",
    "          int(msm.conf * 100), *msm.sample_conf('expectation', markov_average_rg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model is very confident in the prediction of the radius of gyration. However, this does not guarantee it to be accurate -- that is, agree with experimental meaurements. If we lack quantitative agreement with experiments, we can estimate MSMs which optimally balance experimental data and simulation data using the Augmented Markov model (AMM) procedure. A dedicated AMM tutorial can be [found here](http://www.emma-project.org/latest/generated/augmented_markov_model_walkthrough.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Trp-flourescene auto-correlation\n",
    "\n",
    "Fluctuations in tryptophan flourescence can be measured using spectroscopic techniques. These fluctuations depend on (among other things) on the solvent accessible surface area (SASA) of tryptophan residues. Above we pre-computed the SASA using the Shrake-Rupley algorithm. As for the stationary expectation (ensemble averages) considered above, we can use our pre-computed SASA vector to compute the auto-correlation function of trypotophan flourescene using the MSM `correlation` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_time_ml, eq_acf_ml = mlmsm.correlation(markov_average_trp_sasa, maxtime=150)\n",
    "\n",
    "eq_time_bayes, eq_acf_bayes = msm.sample_mean(\n",
    "    'correlation',\n",
    "    np.array(markov_average_trp_sasa),\n",
    "    maxtime=150)\n",
    "\n",
    "eq_acf_bayes_ci_l, eq_acf_bayes_ci_u = msm.sample_conf(\n",
    "    'correlation',\n",
    "    np.array(markov_average_trp_sasa),\n",
    "    maxtime=150)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(eq_time_ml, eq_acf_ml, '-o', color='C1', label='ML MSM')\n",
    "ax.plot(\n",
    "    eq_time_bayes,\n",
    "    eq_acf_bayes,\n",
    "    '--x',\n",
    "    color='C0',\n",
    "    label='Bayes sample mean')\n",
    "ax.fill_between(\n",
    "    eq_time_bayes,\n",
    "    eq_acf_bayes_ci_l[1],\n",
    "    eq_acf_bayes_ci_u[1],\n",
    "    facecolor='C0',\n",
    "    alpha=0.3)\n",
    "ax.semilogx()\n",
    "\n",
    "ax.set_xlim((eq_time_ml[1], eq_time_ml[-1]))\n",
    "ax.set_xlabel(r'time / $\\mathrm{ns}$')\n",
    "ax.set_ylabel(r'Trp-1 SASA ACF / $\\mathrm{nm}^4$')\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the scale on the $y$-axis: this amplitude is likely too small to be experimentally measurable if we consider experimental uncertainty. However, using more advanced experimental setups such as stopped flow, T-jump, P-jump or and others we can prepare our ensemble in a non-equilibrium initial condition. Let us say we can experimentally prepare a our sample to only be in meta-stable state 0. In this case the initial condition will be given by the meta-stable distribution of meta-stable state 0, $p_0$. With `PyEMMA` we can simulate the relaxation from such a non-equilibrium initial condition, $p_0$, back to equilibrium using the `relaxation` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_time_ml, eq_acf_ml = mlmsm.relaxation(\n",
    "    msm.metastable_distributions[0],\n",
    "    markov_average_trp_sasa,\n",
    "    maxtime=150)\n",
    "\n",
    "eq_time_bayes, eq_acf_bayes = msm.sample_mean(\n",
    "    'relaxation',\n",
    "    msm.metastable_distributions[0],\n",
    "    np.array(markov_average_trp_sasa),\n",
    "    maxtime=150)\n",
    "\n",
    "eq_acf_bayes_CI_l, eq_acf_bayes_CI_u = msm.sample_conf(\n",
    "    'relaxation', \n",
    "    msm.metastable_distributions[0],\n",
    "    np.array(markov_average_trp_sasa),\n",
    "    maxtime=150)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(eq_time_ml, eq_acf_ml, '-o', color='C1', label='ML MSM')\n",
    "ax.plot(\n",
    "    eq_time_bayes,\n",
    "    eq_acf_bayes,\n",
    "    '--x',\n",
    "    color='C0',\n",
    "    label='Bayes sample mean')\n",
    "ax.fill_between(\n",
    "    eq_time_bayes,\n",
    "    eq_acf_bayes_CI_l[1],\n",
    "    eq_acf_bayes_CI_u[1],\n",
    "    facecolor='C0',\n",
    "    alpha=0.3)\n",
    "ax.semilogx()\n",
    "\n",
    "ax.set_xlim((eq_time_ml[1], eq_time_ml[-1]))\n",
    "ax.set_xlabel(r'time / $\\mathrm{ns}$')\n",
    "ax.set_ylabel(r'Average Trp-1 SASA / $\\mathrm{nm}^2$')\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This signal is much stronger than what we got from the auto-correlation at equilibrium above! -- If we compute the average observable of each of our meta-stable states and compare them to the ensemble average, we can get an idea of which initial distribution would give us the strongest signal. We compare these values and report their absolute differences below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('    0     1     2     3')\n",
    "print('-----------------------')\n",
    "print('{:.3f} {:.3f} {:.3f} {:.3f} '.format(*np.abs(msm.expectation(markov_average_trp_sasa) - \n",
    "             msm.metastable_distributions.dot(np.array(markov_average_trp_sasa)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how preparing our system in meta-stable state 0 (as we did above!) will give us the strongest signal signified by the largest value of the meta-stable distribution. However, note that there may be other initial conditions which may give us even stronger signals. \n",
    "\n",
    "Using methods such as the ones shown above we can imagine using MSMs to design experiments to aid validation and testing our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling manuscript figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import LogLocator\n",
    "from matplotlib.cm import get_cmap\n",
    "from pyemma.plots.markovtests import _add_ck_subplot\n",
    "\n",
    "mpl.rcParams['axes.titlesize'] = 6\n",
    "mpl.rcParams['axes.labelsize'] = 6\n",
    "#mpl.rcParams['font.size'] = 8\n",
    "mpl.rcParams['legend.fontsize'] = 5\n",
    "mpl.rcParams['xtick.labelsize'] = 5\n",
    "mpl.rcParams['ytick.labelsize'] = 5\n",
    "mpl.rcParams['xtick.minor.pad'] = 2\n",
    "mpl.rcParams['xtick.major.pad'] = 3\n",
    "mpl.rcParams['ytick.minor.pad'] = 2\n",
    "mpl.rcParams['ytick.major.pad'] = 3\n",
    "mpl.rcParams['axes.labelpad'] = 1\n",
    "mpl.rcParams['lines.markersize'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3.47, 6.90))\n",
    "gw = int(np.floor(0.5 + 1000 * fig.get_figwidth()))\n",
    "gh = int(np.floor(0.5 + 1000 * fig.get_figheight()))\n",
    "gs = plt.GridSpec(gh, gw)\n",
    "gs.update(hspace=0.0, wspace=0.0, left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "\n",
    "ax_box = fig.add_subplot(gs[:, :])\n",
    "#ax_box.set_xticks([])\n",
    "#ax_box.set_yticks([])\n",
    "ax_box.set_axis_off()\n",
    "ax_box.text(0.00, 0.98, '(a)', size=10, zorder=1)\n",
    "ax_box.text(0.00, 0.72, '(b)', size=10)\n",
    "ax_box.text(0.55, 0.72, '(c)', size=10)\n",
    "ax_box.text(0.00, 0.47, '(d)', size=10)\n",
    "ax_box.text(0.00, 0.33, '(e)', size=10)\n",
    "ax_box.text(0.00, 0.15, '(f)', size=10)\n",
    "\n",
    "ax_mol = fig.add_subplot(gs[:1713, -3200:])\n",
    "ax_mol.set_axis_off()\n",
    "ax_mol.imshow(\n",
    "    plt.imread(\n",
    "        mdshare.fetch(\n",
    "            'pentapeptide-320.png',\n",
    "            working_directory='data')))\n",
    "\n",
    "ax_feat = fig.add_subplot(gs[2000:3150, 400:1800])\n",
    "ax_feat.bar(\n",
    "    vamp_bars_plot['labels'],\n",
    "    vamp_bars_plot['scores'],\n",
    "    yerr=vamp_bars_plot['errors'],\n",
    "    color=['C0', 'C1', 'C2'])\n",
    "ax_feat.tick_params(axis='x', labelrotation=20)\n",
    "ax_feat.set_ylabel('VAMP2 score')\n",
    "ax_feat.set_title(r'lag time $\\tau$ = {:.1f} ns'.format(vamp_bars_plot['lag'] * 0.1))\n",
    "\n",
    "ax_density = fig.add_subplot(gs[2000:3150, 2200:3350])\n",
    "_, _, misc = pyemma.plots.plot_density(\n",
    "    *tica_concatenated.T[:2],\n",
    "    ax=ax_density,\n",
    "    cax=fig.add_subplot(gs[1900:1950, 2200:3350]),\n",
    "    cbar_orientation='horizontal',\n",
    "    logscale=True)\n",
    "misc['cbar'].set_ticks(LogLocator(base=10.0))\n",
    "misc['cbar'].ax.xaxis.set_ticks_position('top')\n",
    "misc['cbar'].ax.xaxis.set_label_position('top')\n",
    "ax_density.set_xlabel('IC 1')\n",
    "ax_density.set_ylabel('IC 2')\n",
    "\n",
    "x = 0.1 * np.arange(tica_output[0].shape[0])\n",
    "ax_tic1 = fig.add_subplot(gs[3650:4000, 400:3350])\n",
    "ax_tic2 = fig.add_subplot(gs[4000:4350, 400:3350])\n",
    "\n",
    "ax_tic1.plot(x, tica_output[0][:, 0])\n",
    "ax_tic2.plot(x, tica_output[0][:, 1])\n",
    "ax_tic1.set_ylabel('IC 1')\n",
    "ax_tic2.set_ylabel('IC 2')\n",
    "ax_tic2.set_xlabel('time / ns')\n",
    "\n",
    "ax_its = fig.add_subplot(gs[4680:5480, 400:3350])\n",
    "pyemma.plots.plot_implied_timescales(its, units='ns', dt=0.1, ax=ax_its, nits=4, ylog=True)\n",
    "ax_its.set_ylim(1, ax_its.get_ylim()[1])\n",
    "ax_its.set_xlabel(r'lag time $\\tau$ / ns')\n",
    "\n",
    "ax_ck = [\n",
    "    fig.add_subplot(gs[5940:6640, 400:1100]),\n",
    "    fig.add_subplot(gs[5940:6640, 1150:1850]),\n",
    "    fig.add_subplot(gs[5940:6640, 1900:2600]),\n",
    "    fig.add_subplot(gs[5940:6640, 2650:3350])]\n",
    "\n",
    "for k, ax in enumerate(ax_ck):\n",
    "    lest, lpred = _add_ck_subplot(cktest, ax, k, k, ipos=cktest.nsets - 1, dt=0.1, units='ns')\n",
    "    if k > 0:\n",
    "        ax.set_yticks([])\n",
    "predlabel = 'predict ({:3.1f}%)'.format(100.0 * cktest.conf)\n",
    "estlabel = 'estimate'\n",
    "ax_ck[-1].legend(\n",
    "    (lest[0], lpred[0]),\n",
    "    (estlabel, predlabel),\n",
    "    frameon=False,\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(-0.15, 1.35))\n",
    "\n",
    "fig.savefig('figure_1.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3.47, 4))\n",
    "gw = int(np.floor(0.5 + 1000 * fig.get_figwidth()))\n",
    "gh = int(np.floor(0.5 + 1000 * fig.get_figheight()))\n",
    "gs = plt.GridSpec(gh, gw)\n",
    "gs.update(hspace=0.0, wspace=0.0, left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "\n",
    "ax_box = fig.add_subplot(gs[:, :])\n",
    "ax_box.set_axis_off()\n",
    "ax_box.text(0.00, 0.97, '(a)', size=10)\n",
    "ax_box.text(0.52, 0.97, '(b)', size=10)\n",
    "ax_box.text(0.00, 0.50, '(c)', size=10)\n",
    "ax_box.text(0.52, 0.50, '(d)', size=10)\n",
    "\n",
    "ax_fe = fig.add_subplot(gs[400:1750, 400:1750])\n",
    "_, _, misc = pyemma.plots.plot_free_energy(\n",
    "    *tica_concatenated.T[:2],\n",
    "    weights=np.concatenate(msm.trajectory_weights()),\n",
    "    ax=ax_fe,\n",
    "    cax=fig.add_subplot(gs[300:350, 400:1750]),\n",
    "    cbar_orientation='horizontal',\n",
    "    legacy=False)\n",
    "misc['cbar'].set_ticks(np.linspace(0, 8, 5))\n",
    "misc['cbar'].ax.xaxis.set_ticks_position('top')\n",
    "misc['cbar'].ax.xaxis.set_label_position('top')\n",
    "ax_fe.set_ylabel('IC 2')\n",
    "ax_fe.set_xticklabels([])\n",
    "\n",
    "ax_state = fig.add_subplot(gs[400:1750, 2000:3350])\n",
    "_, _, misc = pyemma.plots.plot_state_map(\n",
    "    *tica_concatenated.T[:2],\n",
    "    metastable_traj,\n",
    "    ax=ax_state,\n",
    "    cax=fig.add_subplot(gs[300:350, 2000:3350]),\n",
    "    cbar_label='metastable state',\n",
    "    cbar_orientation='horizontal')\n",
    "misc['cbar'].ax.xaxis.set_ticks_position('top')\n",
    "misc['cbar'].ax.xaxis.set_label_position('top')\n",
    "ax_state.set_xticklabels([])\n",
    "ax_state.set_yticklabels([])\n",
    "\n",
    "evec_idx = 1\n",
    "ax_eig = fig.add_subplot(gs[2300:3650, 400:1750], zorder=1)\n",
    "_, _, misc = pyemma.plots.plot_contour(\n",
    "    *tica_concatenated.T[:2],\n",
    "    eigvec[dtrajs_concatenated, evec_idx],\n",
    "    cmap='PiYG',\n",
    "    ax=ax_eig,\n",
    "    mask=True,\n",
    "    cax=fig.add_subplot(gs[2200:2250, 400:1750]),\n",
    "    cbar_label='{}. right eigenvector'.format(evec_idx + 1),\n",
    "    cbar_orientation='horizontal')\n",
    "misc['cbar'].set_ticks(np.linspace(*misc['cbar'].get_clim(), 3))\n",
    "misc['cbar'].ax.xaxis.set_ticks_position('top')\n",
    "misc['cbar'].ax.xaxis.set_label_position('top')\n",
    "ax_eig.set_xlabel('IC 1')\n",
    "ax_eig.set_ylabel('IC 2')\n",
    "\n",
    "ax_flux = fig.add_subplot(gs[2300:3650, 2000:3350], zorder=1)\n",
    "_, _, misc = pyemma.plots.plot_contour(\n",
    "    *tica_concatenated.T[:2],\n",
    "    flux.committor[dtrajs_concatenated],\n",
    "    cmap='brg',\n",
    "    ax=ax_flux,\n",
    "    mask=True,\n",
    "    cax=fig.add_subplot(gs[2200:2250, 2000:3350]),\n",
    "    cbar_label=r'committor {} $\\to$ {}'.format(start, final),\n",
    "    cbar_orientation='horizontal')\n",
    "misc['cbar'].set_ticks(np.linspace(0, 1, 3))\n",
    "misc['cbar'].set_ticklabels(['start', 'transition state', 'final'])\n",
    "misc['cbar'].ax.xaxis.set_ticks_position('top')\n",
    "misc['cbar'].ax.xaxis.set_label_position('top')\n",
    "ax_flux.set_xlabel('IC 1')\n",
    "ax_flux.set_yticklabels([])\n",
    "\n",
    "fig.savefig('figure_2.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "In this notebook, we have learned how to conduct MD analysis with Markov models. \n",
    "- TODO\n",
    "- discussed some of the caveats that will be further discussed [here ðŸ““](08-common-problems.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
