{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - hidden Markov state models (HMMs)\n",
    "In this notebook, we will learn about hidden Marko state models and how to use them to deal with bad discretization. We also revisit coarse-grained MSMs and realize that they are just HMMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.utils.load_extensions('exercise2/main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import mdshare\n",
    "import pyemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: preprocessed, two-dimensional data (toy model)\n",
    "We load the two-dimensional as well as the true discrete trajectory from an archive using `numpy`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = mdshare.fetch('hmm-doublewell-2d-100k.npz', working_directory='data')\n",
    "with np.load(file) as fh:\n",
    "    data = fh['trajectory']\n",
    "    good_dtraj = fh['discrete_trajectory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and discretize the two-dimensional data poorly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_clustercenters = np.asarray([[-0.1, -0.6], [0.1, 1.4]])\n",
    "poor_dtraj = pyemma.coordinates.assign_to_centers(data, centers=poor_clustercenters)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this poor clustering is depicted in the left panel below, whereas the correct discretization is shown on the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "axes[0].scatter(*data.T, s=1, c=poor_dtraj)\n",
    "axes[0].scatter(*poor_clustercenters.T, s=75, c='red', marker='x')\n",
    "axes[1].scatter(*data.T, s=1, c=good_dtraj)\n",
    "axes[1].scatter(*np.asarray([[0, -1], [0, 1]]).T, s=75, c='red', marker='x')\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "axes[0].set_title('poor discretization')\n",
    "axes[1].set_title('good discretization')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first steps after discretization should always be an implied timescale convergence plot. We try this here for both discretizations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [i + 1 for i in range(10)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "pyemma.plots.plot_implied_timescales(pyemma.msm.its(poor_dtraj, lags=lags, errors='bayes'), ylog=False, ax=axes[0])\n",
    "pyemma.plots.plot_implied_timescales(pyemma.msm.its(good_dtraj, lags=lags, errors='bayes'), ylog=False, ax=axes[1])\n",
    "axes[0].set_title('MSM with poor discretization')\n",
    "axes[1].set_title('MSM with good discretization')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and see that we need a lagtime of at least $4$ or $5$ steps to roughly convergerge the implied timescales for the poor discretization. The good discretization yields a converged timescale from lagtime $1$.\n",
    "\n",
    "We continue to build MSM objects at the respective lagtimes and print out the first implied timescale for both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_msm = pyemma.msm.estimate_markov_model(poor_dtraj, lag=5)\n",
    "good_msm = pyemma.msm.estimate_markov_model(good_dtraj, lag=1)\n",
    "\n",
    "print('MSM (poor): 1. implied timescale = %.2f steps' % poor_msm.timescales()[0])\n",
    "print('MSM (good): 1. implied timescale = %.2f steps' % good_msm.timescales()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the `poor_msm` was estimated from a poorly discretized trajectory, the higher estimation lagtime could mend this issue to some extend and come close to the actual value estimated from a very good discretization.\n",
    "\n",
    "Looking at the next in line validation step, a Chapman-Kolmogorow test, we observe that the `poor_msm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(poor_msm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... as well as the `good_msm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(good_msm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... show excellent agreement between higher lagtime estimation and model prediction.\n",
    "\n",
    "Let us now repeat both estimations using hidden Markov state models instead of regular MSMs. We begin with the implied timescale convergence using the `pyemma.msm.timescales_hmsm()` function and two hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(poor_dtraj, 2, lags=lags, errors='bayes'), ylog=False, ax=axes[0])\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(good_dtraj, 2, lags=lags, errors='bayes'), ylog=False, ax=axes[1])\n",
    "axes[0].set_title('HMM with poor discretization')\n",
    "axes[1].set_title('HMM with good discretization')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, both discretizations give us converged implied timescales from the very start (lagtime $1$).\n",
    "\n",
    "We estimate HMMs using both discretizations at lagtime 1 and two hidden states..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_hmm = pyemma.msm.estimate_hidden_markov_model(poor_dtraj, 2, lag=1)\n",
    "good_hmm = pyemma.msm.estimate_hidden_markov_model(good_dtraj, 2, lag=1)\n",
    "\n",
    "print('HMM (poor): 1. implied timescale = %.2f steps' % poor_hmm.timescales()[0])\n",
    "print('HMM (good): 1. implied timescale = %.2f steps' % good_hmm.timescales()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and obtain nearly identical estimates for the first implied timescale.\n",
    "\n",
    "We observe that HMMs, unlike MSMs, seem to be somewhat resistant to discretization errors.\n",
    "\n",
    "Regarding the CK test, we again see that the `poor_hmm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(poor_hmm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the `good_hmm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(good_hmm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... are in perfect agreement.\n",
    "\n",
    "Let us now worsen the discretization using three badly chosen clustercenters and show the discretization, the MSM-ITS convergence, and the HMM-ITS convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_clustercenters = np.asarray([[-2.5, -1.4], [0.3, 1.2], [2.7, -0.6]])\n",
    "bad_dtraj = pyemma.coordinates.assign_to_centers(data, centers=bad_clustercenters)[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "axes[0].scatter(*data.T, s=1, c=bad_dtraj)\n",
    "axes[0].scatter(*bad_clustercenters.T, s=75, c='red', marker='x')\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.its(bad_dtraj, lags=lags, errors='bayes'), ylog=False, ax=axes[1])\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(bad_dtraj, 2, lags=lags, errors='bayes'), ylog=False, ax=axes[2])\n",
    "axes[0].set_xlabel('$x$')\n",
    "axes[0].set_ylabel('$y$')\n",
    "axes[0].set_title('bad discretization')\n",
    "axes[1].set_title('MSM with bad discretization')\n",
    "axes[2].set_title('HMM with bad discretization')\n",
    "for ax in axes.flat[1:]:\n",
    "    ax.set_ylim(-0.5, 12.5)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three discrete states include data points from the two metastable regions (left panel) and, as the middle panel shows, this discretization error cannot be fixed by using a large lagtime and a regukar MSM estimation.\n",
    "\n",
    "The HMM estimate (right panel) still yields a converged implied timescale, even at lagtime $1$. Also the HMM's CK test passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_hmm = pyemma.msm.estimate_hidden_markov_model(bad_dtraj, 2, lag=1)\n",
    "print('HMM (bad): 1. implied timescale = %.2f steps' % bad_hmm.timescales()[0])\n",
    "pyemma.plots.plot_cktest(bad_hmm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to the techniques shown above, there is another method to obtain an HMM, namely\n",
    "\n",
    "### Coarse-graining an extisting MSM\n",
    "\n",
    "To show how this works, we estimate a regular MSM using the bad discretization and (the unsuitable) lagtime $1$, and print the first implied timescale. Next, we coarse grain this MSM using the `coarse_grain()` method onto two states and print the type of the reulsting object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_msm = pyemma.msm.estimate_markov_model(bad_dtraj, lag=1)\n",
    "print('MSM (bad, normal): 1. implied timescale = %.2f steps' % bad_msm.timescales()[0])\n",
    "\n",
    "bad_coarse_msm = bad_msm.coarse_grain(2)\n",
    "print(type(bad_coarse_msm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our coarse-grained MSM apparently is an HMM which, as we can see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSM (bad, coarse): 1. implied timescale = %.2f steps' % bad_coarse_msm.timescales()[0])\n",
    "\n",
    "pyemma.plots.plot_cktest(bad_coarse_msm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... agrees well with the HMM directly estimated on the bad discretization.\n",
    "\n",
    "## Case 2: low-dimensional molecular dynamics data (alanine dipeptide)\n",
    "We are now illustrating two typical use cases of hidden markov state models: **coarse graining** an existing MSM and **reestimating** an HMM from scratch (to resolve faster processes than an MSM).\n",
    "\n",
    "Let us begin with the (already familiar) coarse graining use case.\n",
    "\n",
    "We fetch the alanine dipeptide data set, load the backbone torsions into memory, directly discretize the full space using $k$-means clustering, visualize the margial and joint distributions of both components as well as the cluster centers, and show the ITS convergence to help selecting a suitable lag time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mdshare.fetch('alanine-dipeptide-nowater.pdb', working_directory='data')\n",
    "files = mdshare.fetch('alanine-dipeptide-*-250ns-nowater.dcd', working_directory='data')\n",
    "\n",
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_backbone_torsions()\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "data_out = np.concatenate(data)\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(data, k=100, max_iter=50, stride=10)\n",
    "its = pyemma.msm.its(cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=4, errors='bayes')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(data_out, feature_labels=['$\\Phi$', '$\\Psi$'], ax=axes[0])\n",
    "axes[1].scatter(*data_out.T, s=1, alpha=0.3)\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=15)\n",
    "axes[1].set_xlabel('$\\Phi$')\n",
    "axes[1].set_ylabel('$\\Psi$')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the implied timescale convergence plot, we choose a lagtime of $10$ steps, estimate a Bayesian MSM, and show the results of a CK test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_msm = pyemma.msm.bayesian_markov_model(cluster.dtrajs, lag=10, dt_traj='0.001 ns')\n",
    "\n",
    "nstates = 4\n",
    "pyemma.plots.plot_cktest(bayesian_msm.cktest(nstates));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a (bayesian) MSM with $100$ discrete states and basic validation. To obtain an HMM with only four states (the number for which we have validated our MSM), we call gthe `coarse_grain()` method and then print some properties of both our MSM and the HMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = bayesian_msm.coarse_grain(nstates)\n",
    "\n",
    "print('MSM: lagtime = %s (%d steps), data timestep = %s' % (\n",
    "    bayesian_msm.dt_model, bayesian_msm.lag, bayesian_msm.dt_traj))\n",
    "print('HMM: lagtime = %s (%d steps), data timestep = %s' % (\n",
    "    hmm.dt_model, hmm.lag, hmm.dt_traj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the HMM has inherited the timestep and lagtime information of the original MSM object.\n",
    "\n",
    "We now plot the stationary distribution (left) of the MSM as well as the metastable memberships (right) taken from the HMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "im = axes[0].scatter(\n",
    "    *data_out.T,\n",
    "    s=1,\n",
    "    c=bayesian_msm.pi[np.concatenate(cluster.dtrajs)])\n",
    "cb = fig.colorbar(im, ax=axes[0])\n",
    "cb.set_label('stationary distribution')\n",
    "im = axes[1].scatter(\n",
    "    *data_out.T,\n",
    "    s=1,\n",
    "    c=hmm.metastable_assignments[np.concatenate(cluster.dtrajs)],\n",
    "    cmap=mpl.cm.get_cmap('viridis', nstates),\n",
    "    norm=mpl.colors.BoundaryNorm(np.arange(-0.5, nstates, 1), nstates))\n",
    "cb = fig.colorbar(im, ax=axes[1])\n",
    "cb.set_label('metastable state')\n",
    "cb.set_ticks(np.arange(nstates))\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('$\\Phi$')\n",
    "    ax.set_ylabel('$\\Psi$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first use case, the HMM helps us to understand which discrete states form metastable sets.\n",
    "For the second use case, we start from the same featurization and a coarse (but reasonable) discretization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = pyemma.coordinates.cluster_regspace(data, dmin=1.0)\n",
    "its = pyemma.msm.its(cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=6, errors='bayes')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(data_out, feature_labels=['$\\Phi$', '$\\Psi$'], ax=axes[0])\n",
    "axes[1].scatter(*data_out.T, s=1, alpha=0.3, c=np.concatenate(cluster.dtrajs))\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=75, c='red', marker='x')\n",
    "axes[1].set_xlabel('$\\Phi$')\n",
    "axes[1].set_ylabel('$\\Psi$')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of building an MSM, we use the ITS plot to guess possible numbers of metastable states and repeat the ITS convergence analysis using (bayesian) HMMs and small lagtimes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(cluster.dtrajs, 4, lags=[1, 2, 3, 4, 5], errors='bayes'), ax=axes[0])\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(cluster.dtrajs, 6, lags=[1, 2, 3, 4, 5], errors='bayes'), ax=axes[1])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left panel shows that an HMM with four hidden states yields converged implied timescales from lagtime $1$.\n",
    "\n",
    "The right panel, however, shows that an HMM with six hiddeen states and lagtime $1$ can resolve two additional processes.\n",
    "\n",
    "Let us follow up on this and perform a CK test for a four state HMM at lagtime $1$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_4 = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 4, lag=1, dt_traj='0.001 ns')\n",
    "pyemma.plots.plot_cktest(hmm_4.cktest(mlags=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and than the six state HMM at laggtime $1$ (we use `mlags=2` because we would loose the two fast processes at lagtimes $\\geq3$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_6 = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 6, lag=1, dt_traj='0.001 ns')\n",
    "pyemma.plots.plot_cktest(hmm_6.cktest(mlags=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the CK test is passed.\n",
    "\n",
    "If we now compare both metastable membership plots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "for hmm, ax in zip([hmm_4, hmm_6], axes.flat):\n",
    "    im = ax.scatter(\n",
    "        *data_out.T,\n",
    "        s=1,\n",
    "        c=hmm.metastable_assignments[np.concatenate(cluster.dtrajs)],\n",
    "        cmap=mpl.cm.get_cmap('viridis', hmm.nstates),\n",
    "        norm=mpl.colors.BoundaryNorm(np.arange(-0.5, hmm.nstates, 1), hmm.nstates))\n",
    "    ax.set_xlabel('$\\Phi$')\n",
    "    ax.set_ylabel('$\\Psi$')\n",
    "    ax.set_title('HMM with %d hidden states' % hmm.nstates)\n",
    "    cb = fig.colorbar(im, ax=ax)\n",
    "    cb.set_label('metastable state')\n",
    "    cb.set_ticks(np.arange(hmm.nstates))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we see that the six state HMM is able to subdivide the two most metastable states of the four state HMM and, tgus, give us a more detailed view on the underlying system.\n",
    "\n",
    "This is the second use case for HMMs: due to the low sensibility to discretization errors, we can afford to estimate HMMs at smaller lagtimes than MSMs and, thus, resolve  more processes.\n",
    "\n",
    "Let us repeat this approach again for another featurization: we already know that it is possible to resolve six metastable states (five slow processes) using an HMM estimated on a coarse discretization of the backbone torsions. Can you achieve the same level of resolution using heavy atom distances and a suitable TICA projection?\n",
    "\n",
    "**Exercise**: obtain the heavy atom distances, use TICA for dimension reduction, and discretize using a method of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "feat = # FIXME\n",
    "feat. # FIXME\n",
    "data = # FIXME\n",
    "\n",
    "tica = # FIXME\n",
    "tica_out = np.concatenate(tica.get_output())\n",
    "\n",
    "cluster = # FIXME\n",
    "its = pyemma.msm.its(cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=4, errors='bayes')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(tica_out, ax=axes[0])\n",
    "axes[1].scatter(*tica_out[:, :2].T, s=1, alpha=0.3)\n",
    "axes[1].scatter(*cluster.clustercenters[:, :2].T, s=15)\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_distances(feat.select_Heavy())\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "\n",
    "tica = pyemma.coordinates.tica(data, lag=3)\n",
    "tica_out = np.concatenate(tica.get_output())\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(tica, k=100, max_iter=50, stride=10)\n",
    "its = pyemma.msm.its(cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=4, errors='bayes')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(tica_out, ax=axes[0])\n",
    "axes[1].scatter(*tica_out[:, :2].T, s=1, alpha=0.3)\n",
    "axes[1].scatter(*cluster.clustercenters[:, :2].T, s=15)\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: let's see if your discretized data is suitable to converge five slow implied timescales using a bayesian HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "pyemma.plots.plot_implied_timescales # FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(cluster.dtrajs, 6, lags=[1, 2, 3, 4], errors='bayes'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: estimate a bayesian HMM and perform a CK test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "hmm = # FIXME\n",
    "pyemma.plots. # FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "hmm = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 6, lag=1, dt_traj='0.001 ns')\n",
    "pyemma.plots.plot_cktest(hmm.cktest(mlags=2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: now that you have a model, be creative and visualize the metastable regions in your projected space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def draw_panel(ax, i, j):\n",
    "    im = ax.scatter(\n",
    "        *tica_out[:, [i, j]].T,\n",
    "        s=1,\n",
    "        c=hmm.metastable_assignments[np.concatenate(cluster.dtrajs)],\n",
    "        cmap=mpl.cm.get_cmap('viridis', 6),\n",
    "        norm=mpl.colors.BoundaryNorm(np.arange(-0.5, 6, 1), 6))\n",
    "    cb = fig.colorbar(im, ax=ax)\n",
    "    cb.set_label('metastable state')\n",
    "    cb.set_ticks(np.arange(6))\n",
    "    ax.set_xlabel('IC %d' % (i + 1))\n",
    "    ax.set_ylabel('IC %d' % (j + 1))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "draw_panel(axes[0, 0], 0, 2)\n",
    "draw_panel(axes[0, 1], 1, 2)\n",
    "draw_panel(axes[1, 0], 0, 1)\n",
    "axes[1, 1].set_axis_off()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "In this notebook, we have learned how to use a hidden Markov state model (HMM) and how they differ from an MSM. In detail, we have used\n",
    "- `pyemma.msm.timescales_hmsm()` function to obtain an implied timescale object for HMMs,\n",
    "- `pyemma.msm.estimate_hidden_markov_model()` to estimate a regular HMM,\n",
    "- `pyemma.msm.bayesian_hidden_markov_model()` to estimate a Bayesian HMM, and\n",
    "- the `metastable_assignments` attribute of an HMM object to access the metastable membership of discrete states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
