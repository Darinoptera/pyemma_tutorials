{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - hidden Markov state models (HMMs)\n",
    "In this notebook, we will learn about hidden Markov state models and how to use them to deal with bad discretization. We further explain how to obtain a coarse model based on an initial MSM analysis.\n",
    "\n",
    "Maintainers:\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import mdshare\n",
    "import pyemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: preprocessed, two-dimensional data (toy model)\n",
    "We load the two-dimensional as well as the true discrete trajectory from an archive using `numpy`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = mdshare.fetch('hmm-doublewell-2d-100k.npz', working_directory='data')\n",
    "with np.load(file) as fh:\n",
    "    data = fh['trajectory']\n",
    "    good_dtraj = fh['discrete_trajectory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and discretize the two-dimensional data poorly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_clustercenters = np.asarray([[-0.1, -0.6], [0.1, 1.4]])\n",
    "poor_dtraj = pyemma.coordinates.assign_to_centers(data, centers=poor_clustercenters)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this poor clustering is depicted in the left panel below, whereas the correct discretization is shown on the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_state_map(*data.T, poor_dtraj, ax=axes[0])\n",
    "axes[0].scatter(*poor_clustercenters.T, s=15, c='C1')\n",
    "pyemma.plots.plot_state_map(*data.T, good_dtraj, ax=axes[1])\n",
    "axes[1].scatter(*np.asarray([[0, -1], [0, 1]]).T, s=15, c='C1')\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    ax.set_aspect('equal')\n",
    "axes[0].set_ylabel('$y$')\n",
    "axes[0].set_title('poor discretization')\n",
    "axes[1].set_title('good discretization')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first steps after discretization should always be an implied timescale convergence plot. We try this here for both discretizations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [i + 1 for i in range(10)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.its(poor_dtraj, lags=lags, errors='bayes', n_jobs=None), ylog=False, ax=axes[0])\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.its(good_dtraj, lags=lags, errors='bayes', n_jobs=None), ylog=False, ax=axes[1])\n",
    "axes[0].set_title('MSM with poor discretization')\n",
    "axes[1].set_title('MSM with good discretization')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and see that we need a lagtime of at least $4$ or $5$ steps to roughly converge the implied timescales for the poor discretization. The good discretization yields a converged timescale from lagtime $1$. For demonstration purposes, let's choose a lagtime $1$ for models of both discretizations and see what happens.\n",
    "\n",
    "We continue to build MSM objects and print out the first implied timescale for both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_msm = pyemma.msm.estimate_markov_model(poor_dtraj, lag=1)\n",
    "good_msm = pyemma.msm.estimate_markov_model(good_dtraj, lag=1)\n",
    "\n",
    "print('MSM (poor): 1. implied timescale = {:.2f} steps'.format(poor_msm.timescales()[0]))\n",
    "print('MSM (good): 1. implied timescale = {:.2f} steps'.format(good_msm.timescales()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the next in line validation step, a Chapman-Kolmogorow test, we observe that the `poor_msm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(poor_msm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... in fact shows deviations that are too large for a simple double well system. In comparison, the `good_msm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(good_msm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... shows excellent agreement between higher lagtime estimation and model prediction.\n",
    "\n",
    "Let us now repeat both estimations using hidden Markov state models instead of regular MSMs. We begin with the implied timescale convergence using the `pyemma.msm.timescales_hmsm()` function and two hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(poor_dtraj, 2, lags=lags, errors='bayes', n_jobs=None),\n",
    "    ylog=False, ax=axes[0])\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(good_dtraj, 2, lags=lags, errors='bayes', n_jobs=None),\n",
    "    ylog=False, ax=axes[1])\n",
    "axes[0].set_title('HMM with poor discretization')\n",
    "axes[1].set_title('HMM with good discretization')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, both discretizations give us converged implied timescales from the very start (lagtime $1$).\n",
    "\n",
    "We estimate HMMs using both discretizations at lagtime 1 and two hidden states..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_hmm = pyemma.msm.estimate_hidden_markov_model(poor_dtraj, 2, lag=1)\n",
    "good_hmm = pyemma.msm.estimate_hidden_markov_model(good_dtraj, 2, lag=1)\n",
    "\n",
    "print('HMM (poor): 1. implied timescale = {:.2f} steps'.format(poor_hmm.timescales()[0]))\n",
    "print('HMM (good): 1. implied timescale = {:.2f} steps'.format(good_hmm.timescales()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and obtain nearly identical estimates for the first implied timescale.\n",
    "\n",
    "We observe that HMMs, unlike MSMs, seem to be somewhat resistant to discretization errors.\n",
    "\n",
    "Regarding the CK test, we again see that the `poor_hmm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(poor_hmm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the `good_hmm`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(good_hmm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... are in perfect agreement.\n",
    "\n",
    "Let us now worsen the discretization using three badly chosen clustercenters and show the discretization, the MSM-ITS convergence, and the HMM-ITS convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_clustercenters = np.asarray([[-2.5, -1.4], [0.3, 1.2], [2.7, -0.6]])\n",
    "bad_dtraj = pyemma.coordinates.assign_to_centers(data, centers=bad_clustercenters)[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_state_map(*data.T, bad_dtraj, ax=axes[0])\n",
    "axes[0].scatter(*bad_clustercenters.T, s=15, c='C1')\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.its(bad_dtraj, lags=lags, errors='bayes'),\n",
    "    ylog=False, ax=axes[1])\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(bad_dtraj, 2, lags=lags, errors='bayes'),\n",
    "    ylog=False, ax=axes[2])\n",
    "axes[0].set_xlabel('$x$')\n",
    "axes[0].set_ylabel('$y$')\n",
    "axes[0].set_xlim(-4, 4)\n",
    "axes[0].set_ylim(-4, 4)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].set_title('bad discretization')\n",
    "axes[1].set_title('MSM with bad discretization')\n",
    "axes[2].set_title('HMM with bad discretization')\n",
    "\n",
    "for ax in axes.flat[1:]:\n",
    "    ax.set_ylim(-0.5, 12.5)\n",
    "    ts = good_msm.timescales()[0]\n",
    "    ax.hlines(ts, *ax.get_xlim(), linestyle=':')\n",
    "    ax.annotate('good_msm', xy=(1.5, 1.05 * ts), ha='left', va='bottom')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three discrete states include data points from the two metastable regions (left panel) and, as the middle panel shows, this discretization error cannot be fixed by using a large lagtime and a regular MSM estimation. The dotted line represents the result from our previously estimated MSM with a fine discretization. \n",
    "\n",
    "The HMM estimate (right panel) still yields a converged implied timescale, even at lagtime $1$. Also the HMM's CK test passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_hmm = pyemma.msm.estimate_hidden_markov_model(bad_dtraj, 2, lag=1)\n",
    "print('HMM (bad): 1. implied timescale = {:.2f} steps'.format(bad_hmm.timescales()[0]))\n",
    "pyemma.plots.plot_cktest(bad_hmm.cktest(2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: low-dimensional molecular dynamics data (alanine dipeptide)\n",
    "We are now illustrating a typical use case of hidden markov state models: estimating an MSM that is used as a heuristics for the number of slow processes or hidden states, and estimating an HMM (to resolve faster processes than an MSM).\n",
    "\n",
    "We fetch the alanine dipeptide data set, load the backbone torsions into memory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mdshare.fetch('alanine-dipeptide-nowater.pdb', working_directory='data')\n",
    "files = mdshare.fetch('alanine-dipeptide-*-250ns-nowater.dcd', working_directory='data')\n",
    "\n",
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_backbone_torsions()\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "data_all = np.concatenate(data)\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(data, k=75, max_iter=50, stride=10)\n",
    "dtrajs_all = np.concatenate(cluster.dtrajs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... discretize the full space using $k$-means clustering, visualize the marginal and joint distributions of both components as well as the cluster centers, and show the ITS convergence to help selecting a suitable lag time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its = pyemma.msm.its(\n",
    "    cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=4, errors='bayes', n_jobs=None)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(data_all, feature_labels=['$\\Phi$', '$\\Psi$'], ax=axes[0])\n",
    "pyemma.plots.plot_density(*data_all.T, ax=axes[1], cbar=False, alpha=0.3)\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=15, c='C1')\n",
    "axes[1].set_xlabel('$\\Phi$')\n",
    "axes[1].set_ylabel('$\\Psi$')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the implied timescale convergence plot, we choose a lagtime of $10$ steps. We further find $3$ slow processes in the implied timescales plot, meaning that we can expect $4$ metastable sets or hidden states. First, we estimate a Bayesian MSM, and show the results of a CK test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_msm = pyemma.msm.bayesian_markov_model(cluster.dtrajs, lag=10, dt_traj='0.001 ns')\n",
    "\n",
    "nstates = 4\n",
    "pyemma.plots.plot_cktest(bayesian_msm.cktest(nstates), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a (bayesian) MSM with $75$ discrete states and basic validation. To obtain an HMM with only four states (the number for which we have validated our MSM), we compute the implied timescales for HMMs with this number of hidden states. \n",
    "\n",
    "We repeat the ITS convergence analysis using (bayesian) HMMs and small lagtimes for a $4$-state HMM. For demonstration purposes, we add the same analysis with a $6$-state HMM to visualize what happens if the number of states is not as clear as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(\n",
    "        cluster.dtrajs, 4, lags=[1, 2, 3, 4, 5], errors='bayes', nsamples=50),\n",
    "    ax=axes[0], units='ps')\n",
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(\n",
    "        cluster.dtrajs, 6, lags=[1, 2, 3, 4, 5], errors='bayes', nsamples=50),\n",
    "    ax=axes[1], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left panel shows that an HMM with four hidden states yields converged implied timescales from lagtime $1$.\n",
    "\n",
    "The right panel, however, shows that an HMM with six hidden states and lagtime $1$ can resolve two additional processes.\n",
    "\n",
    "Let us follow up on this and perform a CK test for a four state HMM at lagtime $1$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_4 = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 4, lag=1, dt_traj='0.001 ns', nsamples=50)\n",
    "pyemma.plots.plot_cktest(hmm_4.cktest(mlags=5), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and than the six state HMM at laggtime $1$ (we use `mlags=2` because we would loose the two fast processes at lagtimes $\\geq3$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_6 = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 6, lag=1, dt_traj='0.001 ns', nsamples=50)\n",
    "pyemma.plots.plot_cktest(hmm_6.cktest(mlags=2), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the CK test is passed.\n",
    "\n",
    "If we now compare both metastable membership plots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "for hmm, ax in zip([hmm_4, hmm_6], axes.flat):\n",
    "    pyemma.plots.plot_state_map(\n",
    "        *data_all.T,\n",
    "        hmm.metastable_assignments[dtrajs_all], \n",
    "        ax=ax)\n",
    "    ax.set_title('HMM with {} hidden states'.format(hmm.nstates))\n",
    "    ax.set_xlabel('$\\Phi$')\n",
    "axes[0].set_ylabel('$\\Psi$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we see that the six state HMM is able to subdivide the two most metastable states of the four state HMM and, thus, give us a more detailed view on the underlying system. As one would have expected from the implied timescale plot, the metastable dynamics is already well-described with $4$ hidden states.\n",
    "\n",
    "Due to the low sensibility to discretization errors, we can afford to estimate HMMs at smaller lagtimes than MSMs and, thus, resolve  more processes.\n",
    "\n",
    "Like with classical MSMs, we can further analyze properties of the HMM. As an example, have a look at the transition paths and committor probabilities below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0]\n",
    "B = [3]\n",
    "flux = pyemma.msm.tpt(hmm_4, A, B)\n",
    "\n",
    "highest_membership = hmm_4.metastable_distributions.argmax(1)\n",
    "coarse_state_centers = cluster.clustercenters[hmm_4.observable_set[highest_membership]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note one important difference when operating on metastable sets: Since HMMs operate directly on the metastable sets, we need not compute the flux between the `msm.metastable_sets` but between the lists of macrostate numbers, e.g. instead of `A = msm.metastable_sets[0]` we need `A = [0]`. \n",
    "\n",
    "Let's now visualize the committor as before. Does it look familiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "pyemma.plots.plot_contour(\n",
    "    *data_all.T, flux.committor[hmm_4.metastable_assignments[dtrajs_all]], cmap='brg', ax=ax,\n",
    "    mask=True, method='nearest', cbar_label='committor 0 -> 3', alpha=.8, zorder=-1);\n",
    "\n",
    "pyemma.plots.plot_flux(flux, coarse_state_centers, flux.stationary_distribution, ax=ax, \n",
    "                       show_committor=False, figpadding=0, show_frame=True);\n",
    "\n",
    "ax.set_xlabel('$\\Phi$')\n",
    "ax.set_ylabel('$\\Psi$')\n",
    "ax.set_xlim(data_all[:, 0].min(), data_all[:, 0].max())\n",
    "ax.set_ylim(data_all[:, 1].min(), data_all[:, 1].max())\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see here, in addition to the properties described here, HMMs provide the same analysis tools as MSMs. E.g. eigenvectors and mean first passage times can be extracted as described in previous notebooks. \n",
    "\n",
    "Let us now repeat this approach again for another featurization: we already know that it is possible to resolve six metastable states (five slow processes) using an HMM estimated on a discretization of the backbone torsions. Can you achieve the same level of resolution using heavy atom distances and a suitable TICA projection?\n",
    "\n",
    "**Exercise 1**: obtain the heavy atom distances, use TICA for dimension reduction, and discretize using a method of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "feat = #FIXME\n",
    "feat. #FIXME\n",
    "data = #FIXME\n",
    "\n",
    "tica = #FIXME\n",
    "tica_all = np.concatenate(tica.get_output())\n",
    "\n",
    "cluster = #FIXME\n",
    "dtrajs_all = #FIXME\n",
    "\n",
    "its = pyemma.msm.its(\n",
    "    cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=4, errors='bayes', n_jobs=None)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(tica_all, ax=axes[0])\n",
    "pyemma.plots.plot_density(*tica_all[:, :2].T, ax=axes[1], cbar=False, alpha=0.1)\n",
    "axes[1].scatter(*cluster.clustercenters[:, :2].T, s=15, c='C1')\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_distances(feat.select_Heavy())\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "\n",
    "tica = pyemma.coordinates.tica(data, lag=3)\n",
    "tica_all = np.concatenate(tica.get_output())\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(tica, k=75, max_iter=50, stride=10)\n",
    "dtrajs_all = np.concatenate(cluster.dtrajs)\n",
    "\n",
    "its = pyemma.msm.its(\n",
    "    cluster.dtrajs, lags=[1, 2, 5, 10, 20, 50], nits=4, errors='bayes', n_jobs=None)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "pyemma.plots.plot_feature_histograms(tica_all, ax=axes[0])\n",
    "pyemma.plots.plot_density(*tica_all[:, :2].T, ax=axes[1], cbar=False, alpha=0.1)\n",
    "axes[1].scatter(*cluster.clustercenters[:, :2].T, s=15, c='C1')\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "pyemma.plots.plot_implied_timescales(its, ax=axes[2], units='ps')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: let's see if your discretized data is suitable to converge five slow implied timescales using a bayesian HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "pyemma.plots.plot_implied_timescales #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "pyemma.plots.plot_implied_timescales(\n",
    "    pyemma.msm.timescales_hmsm(\n",
    "        cluster.dtrajs, 6, lags=[1, 2, 3, 4], errors='bayes'), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: estimate a bayesian HMM and perform a CK test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "hmm = #FIXME\n",
    "pyemma.plots. #FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "hmm = pyemma.msm.bayesian_hidden_markov_model(cluster.dtrajs, 6, lag=1, dt_traj='0.001 ns')\n",
    "pyemma.plots.plot_cktest(hmm.cktest(mlags=2), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: now that you have a model, be creative and visualize the metastable regions in your projected space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def draw_panel(ax, i, j):\n",
    "    pyemma.plots.plot_state_map(\n",
    "        *tica_all[:, [i, j]].T,\n",
    "        hmm.metastable_assignments[dtrajs_all],\n",
    "        ax=ax)\n",
    "    ax.set_xlabel('IC {}'.format(i + 1))\n",
    "    ax.set_ylabel('IC {}'.format(j + 1))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "draw_panel(axes[0, 0], 0, 2)\n",
    "draw_panel(axes[0, 1], 1, 2)\n",
    "draw_panel(axes[1, 0], 0, 1)\n",
    "axes[1, 1].set_axis_off()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "In this notebook, we have learned how to use a hidden Markov state model (HMM) and how they differ from an MSM. In detail, we have used\n",
    "- `pyemma.msm.timescales_hmsm()` function to obtain an implied timescale object for HMMs,\n",
    "- `pyemma.msm.estimate_hidden_markov_model()` to estimate a regular HMM,\n",
    "- `pyemma.msm.bayesian_hidden_markov_model()` to estimate a Bayesian HMM, and\n",
    "- the `metastable_assignments` attribute of an HMM object to access the metastable membership of discrete states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
