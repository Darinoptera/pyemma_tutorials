{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Dimension reduction and discretization\n",
    "In this notebook, we will cover how to perform dimension reduction and discretization of molecular dynamics data.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "Maintainers: [@cwehmeyer](https://github.com/cwehmeyer), [@marscher](https://github.com/marscher), [@thempel](https://github.com/thempel), [@psolsson](https://github.com/psolsson)\n",
    "\n",
    "Remember, to\n",
    "- run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;\n",
    "- find the full documentation at [PyEMMA.org](http://www.pyemma.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mdshare\n",
    "import pyemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: preprocessed, two-dimensional data (toy model)\n",
    "We load the two-dimensional trajectory from an archive using `numpy` and visualize the marginal and joint distributions of both components. In order to make the important concept of metastability easier to understand, an excerpt from the original trajectory is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = mdshare.fetch('hmm-doublewell-2d-100k.npz', working_directory='data')\n",
    "with np.load(file) as fh:\n",
    "    data = fh['trajectory']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(data, feature_labels=['$x$', '$y$'], ax=axes[0])\n",
    "for i, dim in enumerate(['y', 'x']): \n",
    "    axes[0].plot(data[:300, 1 - i], np.linspace(-0.2 + i, 0.8 + i, 300), color='C2', alpha=0.6)\n",
    "    axes[0].annotate(\n",
    "        '${}$(time)'.format(dim),\n",
    "        xy=(3, 0.6 + i),\n",
    "        xytext=(3, i),\n",
    "        arrowprops=dict(fc='C2', ec='None', alpha=0.6, width=2))\n",
    "pyemma.plots.plot_density(*data.T, ax=axes[1])\n",
    "axes[1].set_xlabel('$x$')\n",
    "axes[1].set_ylabel('$y$')\n",
    "axes[1].set_xlim(-4, 4)\n",
    "axes[1].set_ylim(-4, 4)\n",
    "axes[1].set_aspect('equal')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the low dimensionality of this data set, we can directly attempt to discretize, e.g. with $k$-means with $100$ centers and a stride of $5$ to reduce the computational effort..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kmeans = pyemma.coordinates.cluster_kmeans(data, k=100, stride=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or with a regspace technique where all centers should have a minimal pairwise distance of $0.5$ units of length..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_regspace = pyemma.coordinates.cluster_regspace(data, dmin=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then, we visualize both sets of centers on top of the joint distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "for ax, cls in zip(axes.flat, [cluster_kmeans, cluster_regspace]):\n",
    "    pyemma.plots.plot_density(*data.T, ax=ax, cbar=False, alpha=0.1)\n",
    "    ax.scatter(*cls.clustercenters.T, s=15, c='C1')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    ax.set_aspect('equal')\n",
    "axes[0].set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed how the $k$-means centers follow the density of the data points while the regspace centers are spread uniformly over the whole area?\n",
    "\n",
    "The main result of a discretization, however, is not the set of centers but the timeseries of discrete states. These are accessible via the `dtrajs` attribute of any clustering object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_kmeans.dtrajs)\n",
    "print(cluster_regspace.dtrajs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each trajectory passed to the clustering object, we get a corresponding discrete trajectory.\n",
    "\n",
    "Instead of discretizing the full (two-dimensional) space, we can attempt to find a one-dimensional subspace which\n",
    "1. describes the slow dynamics of the data set equally well but\n",
    "2. is easier to discretize.\n",
    "\n",
    "One widespread method for dimension reduction is the principal component analysis (PCA) which finds a subspace with maximized variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data, dim=1)\n",
    "pca_output = pca.get_output()\n",
    "print(pca_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique is the time-lagged independent component analysis (TICA) which finds a subspace with maximized autocorrelation. To compute the autocorrelation, we need a time shifted version of the data. This time shift has a default value of 10 steps, but it is always a good idea to specify it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tica = pyemma.coordinates.tica(data, dim=1, lag=1)\n",
    "tica_output = tica.get_output()\n",
    "print(tica_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamp = pyemma.coordinates.vamp(data, dim=1, lag=1)\n",
    "vamp_output = vamp.get_output()\n",
    "print(vamp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many cases where PCA can find a suitable subspace, there are also many cases where the PCA-based subspace neglects the slow dynamics. In our example, the slow process is the jump between both wells along the $y$ axis while the $x$-axis contains only random noise. For both, PCA and TICA, we show the distribution after projecting the full dynamics onto a one-dimensional subspace (left) and the direction of projection (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_concatenated = pca_output[0]\n",
    "tica_concatenated = tica_output[0]\n",
    "vamp_concatenated = vamp_output[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    np.concatenate([pca_concatenated, tica_concatenated, vamp_concatenated], axis=1),\n",
    "    feature_labels=['PCA', 'TICA', 'VAMP'],\n",
    "    ax=axes[0])\n",
    "pyemma.plots.plot_density(*data.T, ax=axes[1], cbar=False, alpha=0.1)\n",
    "axes[1].plot(\n",
    "    [0, 3 * pca.eigenvectors[0, 0]],\n",
    "    [0, 3 * pca.eigenvectors[1, 0]],\n",
    "    linewidth=3,\n",
    "    label='PCA')\n",
    "axes[1].plot(\n",
    "    [0, 3 * tica.eigenvectors[0, 0]],\n",
    "    [0, 3 * tica.eigenvectors[1, 0]],\n",
    "    linewidth=3,\n",
    "    label='TICA')\n",
    "axes[1].plot(\n",
    "    [0, 3 * vamp.singular_vectors_right[0, 0]],\n",
    "    [0, 3 * vamp.singular_vectors_right[1, 0]],\n",
    "    linewidth=3,\n",
    "    label='VAMP')\n",
    "axes[1].set_xlabel('$x$')\n",
    "axes[1].set_ylabel('$y$')\n",
    "axes[1].set_xlim(-4, 4)\n",
    "axes[1].set_ylim(-4, 4)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that TICA and VAMP project along the $y$-axis and, thus, yields a subspace which clearly resolves both metastable states. PCA on the other hand projects closely along the $x$-axis and does not resolve both metastable states. This is a case in point where variance maximization does not lead to a subspace which resolves the relevant dynamics of the system.\n",
    "\n",
    "This effect can also be seen when we plot the subspace timeseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.plot(pca_concatenated[:300], label='PCA')\n",
    "ax.plot(tica_concatenated[:300], label='TICA')\n",
    "# note that for better comparability, we enforce the same direction as TICA\n",
    "ax.plot(vamp_concatenated[:300] * -1, label='VAMP')\n",
    "ax.set_xlabel('time / steps')\n",
    "ax.set_ylabel('feature values')\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of TICA, we observe that the projected coordinate jumps between two clearly separated plateaus. For PCA, we observe only random fluctuations without any hint of metastablility.\n",
    "\n",
    "For the discretization of both projections, we show an alternative to $k$-means and regspace clustering, the manual creation of a one-dimensional grid with uniform distance between neighbouring centers.\n",
    "\n",
    "We first discretize the PCA projection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_pca = np.linspace(np.min(pca_concatenated), np.max(pca_concatenated), 50)\n",
    "pyemma.coordinates.assign_to_centers(pca, centers=centers_pca.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then the TICA projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_tica = np.linspace(np.min(tica_concatenated), np.max(tica_concatenated), 50)\n",
    "pyemma.coordinates.assign_to_centers(tica, centers=centers_tica.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: low-dimensional molecular dynamics data (alanine dipeptide)\n",
    "We fetch the alanine dipeptide data set, load the backbone torsions into memory, and visualize the margial and joint distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mdshare.fetch('alanine-dipeptide-nowater.pdb', working_directory='data')\n",
    "files = mdshare.fetch('alanine-dipeptide-*-250ns-nowater.dcd', working_directory='data')\n",
    "\n",
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_backbone_torsions()\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "data_concatenated = np.concatenate(data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    np.concatenate(data), feature_labels=['$\\Phi$', '$\\Psi$'], ax=axes[0])\n",
    "pyemma.plots.plot_density(*data_concatenated.T, ax=axes[1])\n",
    "axes[1].set_xlabel('$\\Phi$')\n",
    "axes[1].set_ylabel('$\\Psi$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the previous example, we perform a $k$-means ($100$ centers, stride of 5) and a regspace ($0.3$ radians center distance) on the full two-dimensional data set and visualize the obtained centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kmeans = pyemma.coordinates.cluster_kmeans(data, k=100, max_iter=50, stride=5)\n",
    "cluster_regspace = pyemma.coordinates.cluster_regspace(data, dmin=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "for ax, cls in zip(axes.flat, [cluster_kmeans, cluster_regspace]):\n",
    "    pyemma.plots.plot_density(*data_concatenated.T, ax=ax, cbar=False, alpha=0.1)\n",
    "    ax.scatter(*cls.clustercenters.T, s=15, c='C1')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, notice the difference between $k$-means and regspace clustering.\n",
    "\n",
    "Now, we use a different featurization for the same data set and revisit how to use PCA and TICA.\n",
    "\n",
    "### Exercise 1: data loading \n",
    "Load the heavy atoms' positions into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat. #FIXME\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "\n",
    "print('We have {} features.'.format(feat.dimension()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "pyemma.plots.plot_feature_histograms(np.concatenate(data), feature_labels=feat, ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_selection(feat.select_Heavy())\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "\n",
    "print('We have {} features.'.format(feat.dimension()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "pyemma.plots.plot_feature_histograms(np.concatenate(data), feature_labels=feat, ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretizing a $30$-dimensional feature space is impractical. Let's use PCA to find a low-dimensional projection and visualize the marginal distributions of all principal components (PCs) as well as the joint distributions for the first two PCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data)\n",
    "pca_concatenated = np.concatenate(pca.get_output())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharex=True)\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_concatenated, ['PC {}'.format(i + 1) for i in range(pca.dimension())], ax=axes[0])\n",
    "pyemma.plots.plot_density(*pca_concatenated[:, :2].T, ax=axes[1], cbar=False)\n",
    "pyemma.plots.plot_free_energy(*pca_concatenated[:, :2].T, ax=axes[2], legacy=False)\n",
    "for ax in axes.flat[1:]:\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default parameters, PCA will return as many dimensions as necessary to explain $95\\%$ of the variance; in this case, we have found a five-dimensional subspace which does seem to resolve some metastability in the first three principal components.\n",
    "\n",
    "### Exercise 2: TICA visualization\n",
    "Apply TICA and visualize the marginal distributions of all independent components (ICs) as well as the joint distributions of the first two ICs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "tica =  #FIXME\n",
    "tica_concatenated = np.concatenate(tica.get_output())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_concatenated, ['IC {}'.format(i + 1) for i in range(tica.dimension())], ax=axes[0])\n",
    "pyemma.plots.plot_density(*tica_concatenated[:, :2].T, ax=axes[1], cbar=False)\n",
    "pyemma.plots.plot_free_energy(*tica_concatenated[:, :2].T, ax=axes[2], legacy=False)\n",
    "for ax in axes.flat[1:]:\n",
    "    ax.set_xlabel('IC 1')\n",
    "    ax.set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tica = pyemma.coordinates.tica(data)\n",
    "tica_concatenated = np.concatenate(tica.get_output())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_concatenated, ['IC {}'.format(i + 1) for i in range(tica.dimension())], ax=axes[0])\n",
    "pyemma.plots.plot_density(*tica_concatenated[:, :2].T, ax=axes[1], cbar=False)\n",
    "pyemma.plots.plot_free_energy(*tica_concatenated[:, :2].T, ax=axes[2], legacy=False)\n",
    "for ax in axes.flat[1:]:\n",
    "    ax.set_xlabel('IC 1')\n",
    "    ax.set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TICA, by default, uses a lag time of $10$ steps and a kinetic variance cutoff of $95\\%$ to determine the number of ICs. We observe that this projection does resolve some metastability in both ICs.\n",
    "\n",
    "Whether these projections are suitable for building Markov state models, though, remains to be seen in later tests.\n",
    "\n",
    "### Exercise 3: PCA parameters\n",
    "Perform PCA on the heavy atoms' positions data set with a target dimension of two; then discretize the two-dimensional subspace using $k$-means with $100$ centers and a stride of $5$ to reduce the computational effort.\n",
    "\n",
    "**Hint:** Look up the parameters of `pyemma.coordinates.pca()`, especially the `dim` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "pca =  # FIXME\n",
    "pca_concatenated =  # FIXME\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(pca, k=100, max_iter=50, stride=5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_concatenated, ['PC {}'.format(i + 1) for i in range(pca.dimension())], ax=axes[0])\n",
    "pyemma.plots.plot_density(*pca_concatenated.T, ax=axes[1], cbar=False, alpha=0.1)\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=15, c='C1')\n",
    "axes[1].set_xlabel('PC 1')\n",
    "axes[1].set_ylabel('PC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data, dim=2)\n",
    "pca_concatenated = np.concatenate(pca.get_output())\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(pca, k=100, max_iter=50, stride=5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_concatenated, ['PC {}'.format(i + 1) for i in range(pca.dimension())], ax=axes[0])\n",
    "pyemma.plots.plot_density(*pca_concatenated.T, ax=axes[1], cbar=False, alpha=0.1)\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=15, c='C1')\n",
    "axes[1].set_xlabel('PC 1')\n",
    "axes[1].set_ylabel('PC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: TICA parameters\n",
    "Perform TICA at lag time $1$ step on the heavy atoms' positions data set with a target dimension of two; then discretize the two-dimensional subspace using $k$-means with $100$ centers and a stride of $5$ to reduce the computational effort.\n",
    "\n",
    "**Hint:** Look up the parameters of `pyemma.coordinates.tica()`, especially the `dim` and `lag` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "tica =  # FIXME\n",
    "tica_concatenated =  # FIXME\n",
    "\n",
    "cluster =  # FIXME\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_concatenated, ['IC {}'.format(i + 1) for i in range(tica.dimension())], ax=axes[0])\n",
    "pyemma.plots.plot_density(*tica_concatenated.T, ax=axes[1], cbar=False, alpha=0.1)\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=15, c='C1')\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tica = pyemma.coordinates.tica(data, lag=1, dim=2)\n",
    "tica_concatenated = np.concatenate(tica.get_output())\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(tica, k=100, max_iter=50, stride=5)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_concatenated, ['IC {}'.format(i + 1) for i in range(tica.dimension())], ax=axes[0])\n",
    "pyemma.plots.plot_density(*tica_concatenated.T, ax=axes[1], cbar=False, alpha=0.1)\n",
    "axes[1].scatter(*cluster.clustercenters.T, s=15, c='C1')\n",
    "axes[1].set_xlabel('IC 1')\n",
    "axes[1].set_ylabel('IC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed the difference in the first two ICs for lag times $10$ steps vs $1$ step (eg. result of exercise 2 and 3)?\n",
    "\n",
    "## Case 3: another molecular dynamics data set (pentapeptide)\n",
    "\n",
    "We fetch the pentapeptide data set, load several different input features into memory and perform a VAMP estimation/scoring of each. Since we want to evaluate the VAMP score on a disjoint test set, we split the available files into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mdshare.fetch('pentapeptide-impl-solv.pdb', working_directory='data')\n",
    "files = mdshare.fetch('pentapeptide-*-500ns-impl-solv.xtc', working_directory='data')\n",
    "\n",
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_backbone_torsions(cossin=True)\n",
    "feat.add_sidechain_torsions(which='all', cossin=True)\n",
    "\n",
    "train_files = files[:-2]\n",
    "test_file = files[-2]  # last trajectory is our test data set\n",
    "assert set(train_files) & set(test_file) == set()  # ensure test and train sets do not overlap\n",
    "\n",
    "data_torsions = pyemma.coordinates.load(train_files, features=feat)\n",
    "data_torsions_test = pyemma.coordinates.load(test_file, features=feat)\n",
    "\n",
    "feat.active_features = []\n",
    "feat.add_distances_ca()\n",
    "\n",
    "data_dists_ca = pyemma.coordinates.load(train_files, features=feat)\n",
    "data_dists_ca_test = pyemma.coordinates.load(test_file, features=feat)\n",
    "\n",
    "feat.active_features = []\n",
    "feat.add_contacts(feat.select_Heavy())\n",
    "\n",
    "data_contacts = pyemma.coordinates.load(train_files, features=feat)\n",
    "data_contacts_test = pyemma.coordinates.load(test_file, features=feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_for_lag(ax, lag, dim=3):\n",
    "    vamp_torsions = pyemma.coordinates.vamp(data_torsions, lag=lag, dim=dim)\n",
    "    vamp_dist_ca = pyemma.coordinates.vamp(data_dists_ca, lag=lag, dim=dim)\n",
    "    vamp_contacts = pyemma.coordinates.vamp(data_contacts, lag=lag, dim=dim)\n",
    "\n",
    "    vamps = (vamp_torsions, vamp_dist_ca, vamp_contacts)\n",
    "    test_data = (data_torsions_test, data_dists_ca_test, data_contacts_test)\n",
    "    labels = ('torsions', 'CA distances', 'contacts')\n",
    "    for i, (v, test_data) in enumerate(zip(vamps, test_data)):\n",
    "        s = v.score(test_data=test_data)\n",
    "        ax.bar(i, s)\n",
    "    ax.set_title('VAMP2 @ lag = {} ps'.format(lag))\n",
    "    ax.set_xticks(range(len(vamps)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "plot_for_lag(axes[0], 5)\n",
    "plot_for_lag(axes[1], 10)\n",
    "plot_for_lag(axes[2], 20)\n",
    "plot_for_lag(axes[3], 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the small lag time we see that there is a large gap between torsions angles on the one hand and CA distances and contacts on the other hand. For increasing lag times this gap vanishes, but also the overall score is decreasing.\n",
    "Because we have capped the maximum dimension or equivalently the score to contain only the three largest components of the underlying dynamical model, we can expect only a maximum score of three. As we increase the lag time, more of the fast kinetic processes have already decayed. So these are not contributing to the score anymore. \n",
    "\n",
    "We have learned that backbone and sidechain torisions are better suited than the other features to continue building and analyzing a kinetic model, so we will continue with this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concatenated = data_torsions + [data_torsions_test] # concatenate two lists\n",
    "type(data_concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform a principal component analysis (PCA) with default parameters and visualize the marginal distributions of all PCs and the joint distributions of the first two PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data_concatenated, dim=2)\n",
    "pca_concatenated = np.concatenate(pca.get_output())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_concatenated,\n",
    "    ['PC {}'.format(i + 1) for i in range(pca.dimension())],\n",
    "    ax=axes[0])\n",
    "pyemma.plots.plot_density(*pca_concatenated[:, :2].T, ax=axes[1], cbar=False)\n",
    "pyemma.plots.plot_free_energy(*pca_concatenated[:, :2].T, ax=axes[2], legacy=False)\n",
    "for ax in axes.flat[1:]:\n",
    "    ax.set_xlabel('PC 1')\n",
    "    ax.set_ylabel('PC 2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a look at some TICA projections estimated with the lag times given below and, for each lag time, we visualize the marginal distributions of all ICs and the joint distributions of the first two ICs. The actual dimension is determined by the default variance cutoff of 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1, 2, 5, 10, 20, 50]\n",
    "\n",
    "fig, axes = plt.subplots(len(lags), 3, figsize=(10, len(lags) * 3))\n",
    "for i, lag in enumerate(lags):\n",
    "    tica = pyemma.coordinates.tica(data_concatenated, lag=lag)\n",
    "    tica_concatenated = np.concatenate(tica.get_output())\n",
    "    pyemma.plots.plot_feature_histograms(\n",
    "        tica_concatenated,\n",
    "        ['IC {}'.format(i + 1) for i in range(tica.dimension())],\n",
    "        ax=axes[i, 0])\n",
    "    axes[i, 0].set_title('lag time = {} steps'.format(lag))\n",
    "    axes[i, 1].set_title(\n",
    "        'Density, actual dimension = {}'.format(tica.dimension()))\n",
    "    pyemma.plots.plot_density(\n",
    "        *tica_concatenated[:, :2].T, ax=axes[i, 1], cbar=False)\n",
    "    pyemma.plots.plot_free_energy(\n",
    "        *tica_concatenated[:, :2].T, ax=axes[i, 2], legacy=False)\n",
    "for ax in axes[:, 1:].flat:\n",
    "    ax.set_xlabel('IC 1')\n",
    "    ax.set_ylabel('IC 2')\n",
    "axes[0, 2].set_title('Pseudo free energy')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed that increasing the lag time\n",
    "1. leads to a rotation of the projection and\n",
    "2. reduces the number of TICs to explain $95\\%$ (default) of the kinetic variance?\n",
    "\n",
    "Note that, while we can get lower and lower dimensional subspaces with increased lag times, we also loose information from the faster processes.\n",
    "\n",
    "How to choose the optimal lag time for a TICA projection often is a hard problem and there are only heuristic approaches to it. For example you can search for the amount of dimensions where the variance cutoff does not change anymore.\n",
    "\n",
    "### Exercise 5: PCA\n",
    "Perform PCA with target dimension $3$ on the current feature set and discretize the projected space using $k$-means with $100$ centers and a stride of $5$ to reduce the computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "pca =  # FIXME\n",
    "pca_concatenated = np.concatenate(pca.get_output(stride=5))\n",
    "\n",
    "cluster =  # FIXME\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_concatenated, ['PC {}'.format(i + 1) for i in range(pca.dimension())], ax=axes[0, 0])\n",
    "for ax, (i, j) in zip(axes.flat[1:], [[0, 1], [1, 2], [0, 2]]):\n",
    "    pyemma.plots.plot_density(\n",
    "        *pca_concatenated[:, [i, j]].T, ax=ax, cbar=False, alpha=0.1)\n",
    "    ax.scatter(*cluster.clustercenters[:, [i, j]].T, s=15, c='C1')\n",
    "    ax.set_xlabel('PC {}'.format(i + 1))\n",
    "    ax.set_ylabel('PC {}'.format(j + 1))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "pca = pyemma.coordinates.pca(data_concatenated, dim=3)\n",
    "pca_concatenated = np.concatenate(pca.get_output(stride=5))\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(pca, k=100, max_iter=50, stride=5)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    pca_concatenated, ['PC {}'.format(i + 1) for i in range(pca.dimension())], ax=axes[0, 0])\n",
    "for ax, (i, j) in zip(axes.flat[1:], [[0, 1], [1, 2], [0, 2]]):\n",
    "    pyemma.plots.plot_density(*pca_concatenated[:, [i, j]].T, ax=ax, cbar=False, alpha=0.1)\n",
    "    ax.scatter(*cluster.clustercenters[:, [i, j]].T, s=15, c='C1')\n",
    "    ax.set_xlabel('PC {}'.format(i + 1))\n",
    "    ax.set_ylabel('PC {}'.format(j + 1))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: TICA\n",
    "Perform TICA with target dimension $3$ and lag time $20$ steps on the current feature set and discretize the projected space using $k$-means with $100$ centers and a stride of $5$ to reduce the computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "tica =  # FIXME\n",
    "tica_concatenated =  # FIXME\n",
    "\n",
    "cluster =  # FIXME\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_concatenated, ['IC {}'.format(i + 1) for i in range(tica.dimension())], ax=axes[0, 0])\n",
    "for ax, (i, j) in zip(axes.flat[1:], [[0, 1], [1, 2], [0, 2]]):\n",
    "    pyemma.plots.plot_density(\n",
    "        *tica_concatenated[:, [i, j]].T, ax=ax, cbar=False, alpha=0.1)\n",
    "    ax.scatter(*cluster.clustercenters[:, [i, j]].T, s=15, c='C1')\n",
    "    ax.set_xlabel('IC {}'.format(i + 1))\n",
    "    ax.set_ylabel('IC {}'.format(j + 1))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tica = pyemma.coordinates.tica(data_concatenated, dim=3)\n",
    "tica_concatenated = np.concatenate(tica.get_output(stride=5))\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(tica, k=100, max_iter=50, stride=5)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_concatenated, ['IC {}'.format(i + 1) for i in range(tica.dimension())], ax=axes[0, 0])\n",
    "for ax, (i, j) in zip(axes.flat[1:], [[0, 1], [1, 2], [0, 2]]):\n",
    "    pyemma.plots.plot_density(\n",
    "        *tica_concatenated[:, [i, j]].T, ax=ax, cbar=False, alpha=0.1)\n",
    "    ax.scatter(*cluster.clustercenters[:, [i, j]].T, s=15, c='C1')\n",
    "    ax.set_xlabel('IC {}'.format(i + 1))\n",
    "    ax.set_ylabel('IC {}'.format(j + 1))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: VAMP\n",
    "Perform VAMP with target dimension $3$ and lag time $20$ steps on the current feature set and discretize the projected space using $k$-means with $100$ centers and a stride of $5$ to reduce the computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "vamp =  # FIXME\n",
    "vamp_concatenated =  # FIXME\n",
    "\n",
    "cluster =  # FIXME\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_concatenated, ['IC {}'.format(i + 1) for i in range(vamp.dimension())], ax=axes[0, 0])\n",
    "for ax, (i, j) in zip(axes.flat[1:], [[0, 1], [1, 2], [0, 2]]):\n",
    "    pyemma.plots.plot_density(\n",
    "        *vamp_concatenated[:, [i, j]].T, ax=ax, cbar=False, alpha=0.1)\n",
    "    ax.scatter(*cluster.clustercenters[:, [i, j]].T, s=15, c='C1')\n",
    "    ax.set_xlabel('IC {}'.format(i + 1))\n",
    "    ax.set_ylabel('IC {}'.format(j + 1))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "vamp = pyemma.coordinates.vamp(data_concatenated, lag=20, dim=3)\n",
    "vamp_concatenated = np.concatenate(vamp.get_output(stride=5))\n",
    "\n",
    "cluster = pyemma.coordinates.cluster_kmeans(vamp, k=100, max_iter=50, stride=5)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "pyemma.plots.plot_feature_histograms(\n",
    "    tica_concatenated, ['IC {}'.format(i + 1) for i in range(tica.dimension())], ax=axes[0, 0])\n",
    "for ax, (i, j) in zip(axes.flat[1:], [[0, 1], [1, 2], [0, 2]]):\n",
    "    pyemma.plots.plot_density(*vamp_concatenated[:, [i, j]].T, ax=ax, cbar=False, alpha=0.1)\n",
    "    ax.scatter(*cluster.clustercenters[:, [i, j]].T, s=15, c='C1')\n",
    "    ax.set_xlabel('IC {}'.format(i + 1))\n",
    "    ax.set_ylabel('IC {}'.format(j + 1))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "In this notebook, we have learned how to reduce the dimension of molecular simulation data and discretize the projected dynamics with `pyemma`. In detail, we have used\n",
    "- `pyemma.coordinates.pca()` to perform a principal components analysis,\n",
    "- `pyemma.coordinates.tica()` to perform a time-lagged independent component analysis, and\n",
    "- `pyemma.coordinates.vamp()` to analyze the quality of some feature spaces, perform dimension reduction, and\n",
    "- `pyemma.coordinates.cluster_kmeans()` to perform a $k$-means clustering,\n",
    "- `pyemma.coordinates.cluster_regspace()` to perform a regspace clustering, and\n",
    "- `pyemma.coordinates.assign_to_centers()` to map trajectories to user-defined centers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
